{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HistoMIL Preprocessing Notebook\n",
    "\n",
    "This Jupyter notebook is designed to guide users through the process of performing various preprocessing steps on histopathology whole-slide images using HistoMIL. This includes tissue segmentation, patching (tiling), and feature extraction. All preprocessing steps will be performed in batch. Predefined preprocessing parameters can be found in the HistoMIL package and can be modified in this notebook.\n",
    "\n",
    "Additionally, this notebook will demonstrate how to perform preprocessing steps on a single slide file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Before proceeding with this notebook, please make sure that you have followed the setup instructions provided in the project's README file. This includes creating a conda environment and installing the required dependencies.\n",
    "\n",
    "## Batch Preprocessing\n",
    "\n",
    "The batch preprocessing pipeline in HistoMIL consists of the following steps:\n",
    "\n",
    "Tissue segmentation\n",
    "Patching (tiling)\n",
    "Feature extraction\n",
    "The default preprocessing parameters can be found in the HistoMIL/EXP/paras/slides.py file. You can modify these parameters to customize the preprocessing pipeline for your specific needs.\n",
    "\n",
    "To perform batch preprocessing, you can use the cohort_slide_preprocessing function in the Experiment.cohort_slide_preprocessing module (HistoMIL.EXP.workspace.experiment.Experiment). Here's an example of how to run batch pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set HistoMIL in PATH or change directory to where HistoMIL is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/awxlong/Desktop/my-studies/hpc_exps/') # 'path/to/ parent dir of HistoMIL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/t9d5_kvd3kb4yljp4f0pvqqc0000gn/T/ipykernel_2170/3058447535.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/Users/awxlong/anaconda3/envs/biomedai/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# avoid pandas warning\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# avoid multiprocessing problem\n",
    "import torch\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "#------>stop skimage warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import imageio.core.util\n",
    "import skimage \n",
    "def ignore_warnings(*args, **kwargs):\n",
    "    pass\n",
    "imageio.core.util._precision_warn = ignore_warnings\n",
    "\n",
    "#set logger as INFO\n",
    "from HistoMIL import logger\n",
    "import logging\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "import pickle\n",
    "import timm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HistoMIL.EXP.paras.env import EnvParas\n",
    "from HistoMIL.EXP.workspace.experiment import Experiment\n",
    "from HistoMIL import logger\n",
    "import logging\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------> parameters for reading data\n",
    "\n",
    "preprocess_env = EnvParas()\n",
    "preprocess_env.exp_name = \"wandb exp name\"      # e.g. \"debug_preprocess\"\n",
    "preprocess_env.project = \"wandb project name\"   # e.g. \"test-project\" \n",
    "preprocess_env.entity =  \"wandb entity name\"    # make sure it's initialized to an existing wandb entity\n",
    "\n",
    "#----------------> cohort\n",
    "# you can find more options in HistoMIL/EXP/paras/cohort.py\n",
    "preprocess_env.cohort_para.localcohort_name = \"COAD\" # cohort name, e.g. 'BRCA'\n",
    "preprocess_env.cohort_para.task_name = \"g0_arrest\"     # biomarker name, e.g., 'g0_arrest' and HAS TO COINCIDE with column name\n",
    "preprocess_env.cohort_para.cohort_file = f'local_cohort_{preprocess_env.cohort_para.localcohort_name}.csv' # e.g. local_cohort_BRCA.csv, this is created automatically, and contains folder, filename, slide_nb, tissue_nb, etc. \n",
    "preprocess_env.cohort_para.task_file = f'{preprocess_env.cohort_para.localcohort_name}_{preprocess_env.cohort_para.task_name}.csv' # e.g. BRCA_g0_arrest.csv, which has PatientID matched with g0_arrest labels. This is SUPPLIED by the user and assumed to be stored in the EXP/Data/ directory\n",
    "preprocess_env.cohort_para.pid_name = \"PatientID\"           # default column for merging tables\n",
    "preprocess_env.cohort_para.targets = ['g0_arrest']  # ['name of target_label column'] e.g.  [\"g0_arrest\"]  # the column name of interest # supply as a list of biomarkers\n",
    "preprocess_env.cohort_para.targets_idx = 0                  \n",
    "preprocess_env.cohort_para.label_dict =\"{'negative':0,'positive':1}\"  # SINGLE quotations for the keys, converts strings objects to binary values\n",
    "# preprocess_env.cohort_para.task_additional_idx = [\"g0_score\"] # if CRC_g0_arrest.csv has other biomarkers of interest, name them in this variable, default None. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write a sample task cohort file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Output saved to /Users/awxlong/Desktop/my-studies/hpc_exps/Data/COAD_g0_arrest.csv\n"
     ]
    }
   ],
   "source": [
    "# Input and output file names\n",
    "input_file = '/Users/awxlong/Desktop/my-studies/hpc_exps/HistoMIL/gdc_manifest.2024-06-18.txt' # path to the manifest.txt file which has filenames of diagnostic slides downloaded from TCGA\n",
    "output_file = f'/Users/awxlong/Desktop/my-studies/hpc_exps/Data/{preprocess_env.cohort_para.task_file}' # path to Data/ dir inside the experiment directory where these cohort .csv files are stored \n",
    "# Read the input file and process the filenames\n",
    "with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "    reader = csv.reader(infile, delimiter='\\t')  # Assuming tab-separated values\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Write header to the output file\n",
    "    writer.writerow([preprocess_env.cohort_para.pid_name, preprocess_env.cohort_para.task_name])\n",
    "    \n",
    "    # Skip the header row of the input file\n",
    "    next(reader)\n",
    "    \n",
    "    # Process each row\n",
    "    for row in reader:\n",
    "        if len(row) >= 3:  # Ensure the row has at least 3 columns\n",
    "            filename = row[1]  # The filename is in the third column (index 2)\n",
    "            index = filename[:12]  # Take the first 12 characters as the index\n",
    "            even_odd = 'positive' if int(row[3])%2 == 0 else 'negative'\n",
    "            writer.writerow([index, even_odd])\n",
    "            \n",
    "print(f\"Processing complete. Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #----------------> model specifications for preprocessing\n",
    "# slide-level parameters\n",
    "print(preprocess_env.collector_para.slide)\n",
    "\n",
    "# tissue-level parameters\n",
    "print(preprocess_env.collector_para.tissue)\n",
    "\n",
    "# patch-level parameters\n",
    "preprocess_env.collector_para.patch.step_size = int('your step size for patching') # e.g. 224 # ASSUME this also decides the size of patch, although you can change this\n",
    "preprocess_env.collector_para.patch.patch_size = (int('your step size for patching'), int('your step size for patching')) # can change this, default is 512, 512\n",
    "print(preprocess_env.collector_para.patch)\n",
    "\n",
    "# feature-extraction parameters\n",
    "# by default uses resnet18\n",
    "BACKBONES = {\n",
    "    'UNI': \"Not Implemented\",\n",
    "    'prov-gigapath' : timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=True)\n",
    "}\n",
    "backbone_name = None # 'name of feature extractor, e.g. prov-gigapath. If none, by default HistoMIL uses resnet18'\n",
    "if backbone_name:\n",
    "    preprocess_env.collector_para.feature.model_name = backbone_name               # e.g. 'prov-gigapath'\n",
    "    preprocess_env.collector_para.feature.model_instance = BACKBONES[backbone_name] # timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=True)\n",
    "print(preprocess_env.collector_para.feature)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------------> dataset -- > not sure what this is \n",
    "preprocess_env.dataset_para.dataset_name = 'dataset name' # e.g. \"DNAD_L2\"\n",
    "preprocess_env.dataset_para.concepts = 'concepts you want to use'    # default ['slide', 'tissue', 'patch', 'feature'] in this ORDER\n",
    "preprocess_env.dataset_para.split_ratio = 'split ratio which sum to one'   # e.g [0.99,0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------> init machine and person by reading pkl file from notebook 0\n",
    "\n",
    "machine_cohort_loc = \"Path/to/BRCA_machine_config.pkl\"\n",
    "with open(machine_cohort_loc, \"rb\") as f:   # Unpickling\n",
    "    [data_locs,exp_locs,machine,user] = pickle.load(f)\n",
    "preprocess_env.data_locs = data_locs\n",
    "preprocess_env.exp_locs = exp_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------> setup experiment for preprocessing (no ssl)\n",
    "logger.info(\"setup experiment\")\n",
    "from HistoMIL.EXP.workspace.experiment import Experiment\n",
    "exp = Experiment(env_paras=preprocess_env)\n",
    "exp.setup_machine(machine=machine,user=user)\n",
    "logger.info(\"setup data\")\n",
    "exp.init_cohort()                   # This will create 2 files inside EXP/Data/: local_cohort_BRCA.csv which has filenames of WSIs stored in TCGA-BRCA/ and Task_g0_arrest.csv which merges the local_cohort_BRCA.csv with the supplied BRCA_g0_arrest.csv\n",
    "logger.info(\"pre-processing..\")\n",
    "exp.cohort_slide_preprocessing(concepts=[\"slide\",\"tissue\",\"patch\",\"feature\"],\n",
    "                                is_fast=True, force_calc=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Slide Preprocessing\n",
    "\n",
    "If you want to perform preprocessing steps on a single slide file, you can use the preprocess_slide function in the HistoMIL.DATA.Slide.collector.pre_process_wsi_collector  function. Here's how we define this function and an example of how to use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from HistoMIL.DATA.Slide.collector import WSICollector,CollectorParas\n",
    "from HistoMIL.EXP.paras.slides import DEFAULT_CONCEPT_PARAS\n",
    "def pre_process_wsi_collector(data_locs,\n",
    "                            wsi_loc:Path,\n",
    "                            collector_paras:CollectorParas,\n",
    "                            concepts:list=[\"slide\",\"tissue\",\"patch\"],\n",
    "                            fast_process:bool=True,force_calc:bool=False):\n",
    "\n",
    "    C = WSICollector(db_loc=data_locs,wsi_loc=wsi_loc,paras=collector_paras)\n",
    "    try:\n",
    "\n",
    "        for name in concepts:\n",
    "            if name == \"tissue\":\n",
    "                if fast_process:\n",
    "                    from HistoMIL.EXP.paras.slides import set_min_seg_level\n",
    "                    C.paras.tissue = set_min_seg_level(C.paras.tissue, C.slide,C.paras.tissue.min_seg_level)\n",
    "                    logger.debug(f\"Collector:: set seg level to {C.paras.tissue.seg_level}\")\n",
    "            C.create(name)\n",
    "            C.get(name, force_calc) # for tissue, req_idx_0 is always default slide\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "    else:\n",
    "        logger.info(f\"Collector:: {wsi_loc} is done\")\n",
    "    finally:\n",
    "        del C\n",
    "\n",
    "folder = \"folder of wsi/\"\n",
    "fname =  \"name of wsi.svs\"\n",
    "wsi_loc = Path(str(\"/\"+ folder +\"/\"+ fname))\n",
    "\n",
    "pre_process_wsi_collector(data_locs,\n",
    "                            wsi_loc,\n",
    "                            collector_paras=DEFAULT_CONCEPT_PARAS,\n",
    "                            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Histo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
