{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HistoMIL Preprocessing Notebook\n",
    "\n",
    "This Jupyter notebook is designed to guide users through the process of performing various preprocessing steps on histopathology whole-slide images using HistoMIL. This includes tissue segmentation, patching (tiling), and feature extraction. All preprocessing steps will be performed in batch. Predefined preprocessing parameters can be found in the HistoMIL package and can be modified in this notebook.\n",
    "\n",
    "Additionally, this notebook will demonstrate how to perform preprocessing steps on a single slide file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Before proceeding with this notebook, please make sure that you have followed the setup instructions provided in the project's README file. This includes creating a conda environment and installing the required dependencies.\n",
    "\n",
    "## Batch Preprocessing\n",
    "\n",
    "The batch preprocessing pipeline in HistoMIL consists of the following steps:\n",
    "\n",
    "Tissue segmentation\n",
    "Patching (tiling)\n",
    "Feature extraction\n",
    "The default preprocessing parameters can be found in the HistoMIL/EXP/paras/slides.py file. You can modify these parameters to customize the preprocessing pipeline for your specific needs.\n",
    "\n",
    "To perform batch preprocessing, you can use the cohort_slide_preprocessing function in the Experiment.cohort_slide_preprocessing module (HistoMIL.EXP.workspace.experiment.Experiment). Here's an example of how to run batch pre-processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set HistoMIL in PATH or change directory to where HistoMIL is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('path/to/HistoMIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid pandas warning\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# avoid multiprocessing problem\n",
    "import torch\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "#------>stop skimage warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import imageio.core.util\n",
    "import skimage \n",
    "def ignore_warnings(*args, **kwargs):\n",
    "    pass\n",
    "imageio.core.util._precision_warn = ignore_warnings\n",
    "\n",
    "#set logger as INFO\n",
    "from HistoMIL import logger\n",
    "import logging\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "import pickle\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HistoMIL.EXP.paras.env import EnvParas\n",
    "from HistoMIL.EXP.workspace.experiment import Experiment\n",
    "from HistoMIL import logger\n",
    "import logging\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------> parameters for reading data\n",
    "\n",
    "preprocess_env = EnvParas()\n",
    "preprocess_env.exp_name = \"wandb exp name\"      # e.g. \"debug_preprocess\"\n",
    "preprocess_env.project = \"wandb project name\"   # e.g. \"test-project\" \n",
    "preprocess_env.entity =  \"wandb entity name\"    # make sure it's initialized to an existing wandb entity\n",
    "\n",
    "#----------------> cohort\n",
    "# you can find more options in HistoMIL/EXP/paras/cohort.py\n",
    "preprocess_env.cohort_para.localcohort_name = \"cohort name\" # 'BRCA'\n",
    "preprocess_env.cohort_para.task_name = \"biomarker name\"     # e.g. 'g0_arrest' and HAS TO COINCIDE with column name\n",
    "preprocess_env.cohort_para.cohort_file = f'local_cohort_{preprocess_env.cohort_para.localcohort_name}.csv' # e.g. local_cohort_BRCA.csv, this is created automatically, and contains folder, filename, slide_nb, tissue_nb, etc. \n",
    "preprocess_env.cohort_para.task_file = f'{preprocess_env.cohort_para.localcohort_name}_{preprocess_env.cohort_para.task_name}.csv' # e.g. BRCA_g0_arrest.csv, which has PatientID matched with g0_arrest labels. This is SUPPLIED by the user and assumed to be stored in the EXP/Data/ directory\n",
    "preprocess_env.cohort_para.pid_name = \"PatientID\"           # default column for merging tables\n",
    "preprocess_env.cohort_para.targets = f'name of target_label column'  # e.g. \"g0_arrest\"  # the column name of interest\n",
    "preprocess_env.cohort_para.targets_idx = 0                  \n",
    "preprocess_env.cohort_para.label_dict =\"{'HRD':0,'HRP':1}\"  # SINGLE quotations for the keys, converts strings objects to binary values\n",
    "# preprocess_env.cohort_para.task_additional_idx = [\"g0_score\"] # if CRC_g0_arrest.csv has other biomarkers of interest, name them in this variable, default None. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #----------------> model specifications for preprocessing\n",
    "# slide-level parameters\n",
    "print(preprocess_env.collector_para.slide)\n",
    "\n",
    "# tissue-level parameters\n",
    "print(preprocess_env.collector_para.tissue)\n",
    "\n",
    "# patch-level parameters\n",
    "preprocess_env.collector_para.patch.step_size = int('your step size for patching') # e.g. 224 # ASSUME this also decides the size of patch, although you can change this\n",
    "preprocess_env.collector_para.patch.patch_size = (int('your step size for patching'), int('your step size for patching')) # can change this, default is 512, 512\n",
    "print(preprocess_env.collector_para.patch)\n",
    "\n",
    "# feature-extraction parameters\n",
    "# by default uses resnet18\n",
    "BACKBONES = {\n",
    "    'UNI': \"Not Implemented\",\n",
    "    'prov-gigapath' : timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=True)\n",
    "}\n",
    "backbone_name = None # 'name of feature extractor, e.g. prov-gigapath. If none, by default HistoMIL uses resnet18'\n",
    "if backbone_name:\n",
    "    preprocess_env.collector_para.feature.model_name = backbone_name               # e.g. 'prov-gigapath'\n",
    "    preprocess_env.collector_para.feature.model_instance = BACKBONES[backbone_name] # timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=True)\n",
    "print(preprocess_env.collector_para.feature)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------------> dataset -- > not sure what this is \n",
    "preprocess_env.dataset_para.dataset_name = 'dataset name' # e.g. \"DNAD_L2\"\n",
    "preprocess_env.dataset_para.concepts = 'concepts you want to use'    # default ['slide', 'tissue', 'patch', 'feature'] in this ORDER\n",
    "preprocess_env.dataset_para.split_ratio = 'split ratio which sum to one'   # e.g [0.99,0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------> init machine and person by reading pkl file from notebook 0\n",
    "\n",
    "machine_cohort_loc = \"Path/to/BRCA_machine_config.pkl\"\n",
    "with open(machine_cohort_loc, \"rb\") as f:   # Unpickling\n",
    "    [data_locs,exp_locs,machine,user] = pickle.load(f)\n",
    "preprocess_env.data_locs = data_locs\n",
    "preprocess_env.exp_locs = exp_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------> setup experiment for preprocessing (no ssl)\n",
    "logger.info(\"setup experiment\")\n",
    "from HistoMIL.EXP.workspace.experiment import Experiment\n",
    "exp = Experiment(env_paras=preprocess_env)\n",
    "exp.setup_machine(machine=machine,user=user)\n",
    "logger.info(\"setup data\")\n",
    "exp.init_cohort()                   # This will create 2 files inside EXP/Data/: local_cohort_BRCA.csv which has filenames of WSIs stored in TCGA-BRCA/ and Task_g0_arrest.csv which merges the local_cohort_BRCA.csv with the supplied BRCA_g0_arrest.csv\n",
    "logger.info(\"pre-processing..\")\n",
    "exp.cohort_slide_preprocessing(concepts=[\"slide\",\"tissue\",\"patch\",\"feature\"],\n",
    "                                is_fast=True, force_calc=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Slide Preprocessing\n",
    "\n",
    "If you want to perform preprocessing steps on a single slide file, you can use the preprocess_slide function in the HistoMIL.DATA.Slide.collector.pre_process_wsi_collector  function. Here's how we define this function and an example of how to use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from HistoMIL.DATA.Slide.collector import WSICollector,CollectorParas\n",
    "from HistoMIL.EXP.paras.slides import DEFAULT_CONCEPT_PARAS\n",
    "def pre_process_wsi_collector(data_locs,\n",
    "                            wsi_loc:Path,\n",
    "                            collector_paras:CollectorParas,\n",
    "                            concepts:list=[\"slide\",\"tissue\",\"patch\"],\n",
    "                            fast_process:bool=True,force_calc:bool=False):\n",
    "\n",
    "    C = WSICollector(db_loc=data_locs,wsi_loc=wsi_loc,paras=collector_paras)\n",
    "    try:\n",
    "\n",
    "        for name in concepts:\n",
    "            if name == \"tissue\":\n",
    "                if fast_process:\n",
    "                    from HistoMIL.EXP.paras.slides import set_min_seg_level\n",
    "                    C.paras.tissue = set_min_seg_level(C.paras.tissue, C.slide,C.paras.tissue.min_seg_level)\n",
    "                    logger.debug(f\"Collector:: set seg level to {C.paras.tissue.seg_level}\")\n",
    "            C.create(name)\n",
    "            C.get(name, force_calc) # for tissue, req_idx_0 is always default slide\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "    else:\n",
    "        logger.info(f\"Collector:: {wsi_loc} is done\")\n",
    "    finally:\n",
    "        del C\n",
    "\n",
    "folder = \"folder of wsi/\"\n",
    "fname =  \"name of wsi.svs\"\n",
    "wsi_loc = Path(str(\"/\"+ folder +\"/\"+ fname))\n",
    "\n",
    "pre_process_wsi_collector(data_locs,\n",
    "                            wsi_loc,\n",
    "                            collector_paras=DEFAULT_CONCEPT_PARAS,\n",
    "                            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Histo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
