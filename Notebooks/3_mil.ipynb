{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HistoMIL Multiple Instance Learning Notebook\n",
    "\n",
    "This Jupyter notebook demonstrates how to train a model using multiple instance learning (MIL) on histopathology whole-slide images using HistoMIL. The notebook is divided into three main sections: parameter definition, data preparation, and model definition and training.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Before proceeding with this notebook, please make sure that you have followed the setup instructions provided in the project's README file. This includes creating a conda environment and installing the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/t9d5_kvd3kb4yljp4f0pvqqc0000gn/T/ipykernel_8381/2197810549.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#--------------------------> base env setting\n",
    "# avoid pandas warning\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# avoid multiprocessing problem\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "#--------------------------> logging setup\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='|%(asctime)s.%(msecs)03d| [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d|%H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change path to use HistoMIL since it's not a library that is pip installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/awxlong/Desktop/my-studies/hpc_exps/') # path to parent dir of HistoMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/awxlong/anaconda3/envs/biomedai/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from HistoMIL.MODEL.Image.MIL.TransMIL.paras import TransMILParas\n",
    "from HistoMIL.MODEL.Image.MIL.DSMIL.paras import DSMILParas\n",
    "from HistoMIL.EXP.paras.env import EnvParas\n",
    "\n",
    "import pickle\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "The second section of the notebook covers model definition for MIL. This includes defining the MIL model architecture using the parameters defined in the first section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------> model setting\n",
    "\n",
    "# for transmil\n",
    "model_para_transmil = TransMILParas()\n",
    "model_para_transmil.feature_size=512\n",
    "model_para_transmil.n_classes=2\n",
    "model_para_transmil.norm_layer=nn.LayerNorm\n",
    "# for dsmil\n",
    "model_para_dsmil = DSMILParas()\n",
    "model_para_dsmil.feature_dim = 224 # feature embedding size of feature extractor, in this case prov-gigapath's\n",
    "model_para_dsmil.p_class = 2\n",
    "model_para_dsmil.b_class = 2\n",
    "model_para_dsmil.dropout_r = 0.5\n",
    "\n",
    "model_name = \"TransMIL\"  # or \"TransMIL\" or \"ABMIL\"\n",
    "\n",
    "model_para_settings = {\"TransMIL\":model_para_transmil,\n",
    "                       \"DSMIL\":model_para_dsmil} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_para_transmil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_para_transmil\u001b[49m\u001b[38;5;241m.\u001b[39mencoder_name \u001b[38;5;66;03m# if you already ran preprocessing and stored the feature vectors, ENSURE this is set as 'pre-calculated'\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_para_transmil' is not defined"
     ]
    }
   ],
   "source": [
    "model_para_transmil.encoder_name # if you already ran preprocessing and stored the feature vectors, ENSURE this is set as 'pre-calculated'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Definition\n",
    "\n",
    "The first section of the notebook defines the parameters used in the MIL training process. This includes the model architecture, loss function, optimizer, and learning rate scheduler. You can modify these parameters to customize the training process for your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene2k_env = EnvParas()\n",
    "precomputed = True\n",
    "\n",
    "#--------------------------> task setting\n",
    "task_name = \"g0_arrest\" # Coincides with column name of target label\n",
    "\n",
    "#--------------------------> parameters\n",
    "# logging information\n",
    "gene2k_env.exp_name = f\"{model_name}_{task_name}\"\n",
    "gene2k_env.project = \"g0_arrest\" \n",
    "gene2k_env.entity = \"cell-x\"    # make sure it's initialized to an existing wandb entity\n",
    "\n",
    "#----------------> cohort\n",
    "gene2k_env.cohort_para.localcohort_name = \"CRC\" # name of patient cohort \n",
    "gene2k_env.cohort_para.task_name = task_name\n",
    "gene2k_env.cohort_para.cohort_file = f'local_cohort_{gene2k_env.cohort_para.localcohort_name}.csv' # e.g. local_cohort_CRC.csv, this is created automatically, and contains folder, filename, slide_nb, tissue_nb, etc. \n",
    "gene2k_env.cohort_para.task_file = f'{gene2k_env.cohort_para.localcohort_name}_{gene2k_env.cohort_para.task_name}.csv' # e.g. CRC_g0_arrest.csv, which has PatientID matched with g0_arrest labels. This is SUPPLIED by the user and assumed to be stored in the EXP/Data/ directory\n",
    "gene2k_env.cohort_para.pid_name = \"PatientID\"\n",
    "gene2k_env.cohort_para.targets = ['g0_arrest']  # e.g. \"g0_arrest\"  # the column name of interest; supply as a list\n",
    "gene2k_env.cohort_para.targets_idx = 0\n",
    "gene2k_env.cohort_para.label_dict = {'negative':0,'positive':1}  # SINGLE quotations for the keys, converts strings objects to binary values\n",
    "#debug_env.cohort_para.update_localcohort = True\n",
    "#----------------> pre-processing\n",
    "#----------------> dataset\n",
    "gene2k_env.dataset_para.dataset_name = f\"CRC_{task_name}\"\n",
    "gene2k_env.dataset_para.concepts = [\"slide\",\"patch\",\"feature\"] # default ['slide', 'tissue', 'patch', 'feature'] in this ORDER\n",
    "gene2k_env.dataset_para.split_ratio = [0.8,0.2]                # dataset split ratio which must sum to one, and training ratio is greater than testing\n",
    "#----------------> model\n",
    "gene2k_env.trainer_para.model_name = model_name\n",
    "gene2k_env.trainer_para.model_para = model_para_settings[model_name]\n",
    "#----------------> trainer or analyzer\n",
    "if precomputed:\n",
    "    gene2k_env.trainer_para.use_pre_calculated = True ### FOR LOADING COMPUTED FEATURES\n",
    "else:\n",
    "    gene2k_env.trainer_para.backbone_name = \"resnet18\"\n",
    "    gene2k_env.trainer_para.additional_pl_paras.update({\"accumulate_grad_batches\":8})\n",
    "    gene2k_env.trainer_para.label_format = \"int\"#\"one_hot\" \n",
    "\n",
    "\n",
    "#k_fold = None\n",
    "#--------------------------> init machine and person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_cohort_loc = \"/Users/awxlong/Desktop/my-studies/hpc_exps/User/CRC_machine_config.pkl\"\n",
    "with open(machine_cohort_loc, \"rb\") as f:   # Unpickling\n",
    "    [data_locs,exp_locs,machine,user] = pickle.load(f)\n",
    "gene2k_env.data_locs = data_locs\n",
    "gene2k_env.exp_locs = exp_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize wandb (once is enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # api_dir = 'path/to API.env/'                    # We assume you store your API keys in a .env file\n",
    "# # load_dotenv(dotenv_path=f\"{api_dir}API.env\")\n",
    "# # user.wandb_api_key = os.getenv(\"WANDB_API_KEY\") # We assume your wandb API key is named as WANDB_API_KEY in the API.env file                             # should have the api key if machine_config.ipynb was run without problems\n",
    "# user.wandb_api_key                                # should have the API key if the machine_config.ipynb notebook was run without issues\n",
    "\n",
    "# wandb.setup(settings=wandb.Settings(\n",
    "#     _disable_stats=True,\n",
    "#     disable_git=True,\n",
    "#     api_key=user.wandb_api_key  \n",
    "# ))\n",
    "\n",
    "# wandb.init(project=gene2k_env.project, \n",
    "#            entity=gene2k_env.entity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisation and Training\n",
    "\n",
    "The third and final section of the notebook covers model definition and training. This includes defining the MIL model using the parameters defined in the first section, and training the model using the dataloaders created in the second section.\n",
    "\n",
    "After training is complete, the notebook will also demonstrate how to evaluate the trained model on a validation set and make predictions on new whole-slide images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|2024-06-20|19:01:58.224| [INFO] setup experiment\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gene2k_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mHistoMIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mEXP\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkspace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Experiment\n\u001b[0;32m----> 3\u001b[0m exp \u001b[38;5;241m=\u001b[39m Experiment(env_paras\u001b[38;5;241m=\u001b[39m\u001b[43mgene2k_env\u001b[49m)\n\u001b[1;32m      4\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup_machine(machine\u001b[38;5;241m=\u001b[39mmachine,user\u001b[38;5;241m=\u001b[39muser)\n\u001b[1;32m      5\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gene2k_env' is not defined"
     ]
    }
   ],
   "source": [
    "logging.info(\"setup experiment\")\n",
    "from HistoMIL.EXP.workspace.experiment import Experiment\n",
    "exp = Experiment(env_paras=gene2k_env)\n",
    "exp.setup_machine(machine=machine,user=user)\n",
    "logging.info(\"setup data\")\n",
    "exp.init_cohort()\n",
    "logging.info(\"setup trainer..\")\n",
    "exp.setup_experiment(main_data_source=\"slide\",\n",
    "                    need_train=True)\n",
    "\n",
    "exp.exp_worker.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Histo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
