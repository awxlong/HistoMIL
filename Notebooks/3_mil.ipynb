{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HistoMIL Multiple Instance Learning Notebook\n",
    "\n",
    "This Jupyter notebook demonstrates how to train a model using multiple instance learning (MIL) on histopathology whole-slide images using HistoMIL. The notebook is divided into three main sections: parameter definition, data preparation, and model definition and training.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Before proceeding with this notebook, please make sure that you have followed the setup instructions provided in the project's README file. This includes creating a conda environment and installing the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------> base env setting\n",
    "# avoid pandas warning\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# avoid multiprocessing problem\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "#--------------------------> logging setup\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='|%(asctime)s.%(msecs)03d| [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d|%H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change path to use HistoMIL since it's not a library that is pip installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('path/to/parent-dir of HistoMIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HistoMIL.MODEL.Image.MIL.TransMIL.paras import TransMILParas\n",
    "from HistoMIL.MODEL.Image.MIL.DSMIL.paras import DSMILParas\n",
    "from HistoMIL.EXP.paras.env import EnvParas\n",
    "\n",
    "import pickle\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "The second section of the notebook covers model definition for MIL. This includes defining the MIL model architecture using the parameters defined in the first section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------> model setting\n",
    "\n",
    "# for transmil\n",
    "model_para_transmil = TransMILParas()\n",
    "model_para_transmil.feature_size=512\n",
    "model_para_transmil.n_classes=2\n",
    "model_para_transmil.norm_layer=nn.LayerNorm\n",
    "# for dsmil\n",
    "model_para_dsmil = DSMILParas()\n",
    "model_para_dsmil.feature_dim = 512 #resnet18\n",
    "model_para_dsmil.p_class = 2\n",
    "model_para_dsmil.b_class = 2\n",
    "model_para_dsmil.dropout_r = 0.5\n",
    "\n",
    "model_name = \"TransMIL\"  # or \"TransMIL\" or \"ABMIL\"\n",
    "\n",
    "model_para_settings = {\"TransMIL\":model_para_transmil,\n",
    "                       \"DSMIL\":model_para_dsmil} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_para_transmil.encoder_name # if you already ran preprocessing and stored the feature vectors, ENSURE this is set as 'pre-calculated'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Definition\n",
    "\n",
    "The first section of the notebook defines the parameters used in the MIL training process. This includes the model architecture, loss function, optimizer, and learning rate scheduler. You can modify these parameters to customize the training process for your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene2k_env = EnvParas()\n",
    "\n",
    "\n",
    "#--------------------------> task setting\n",
    "task_name = \"example_mil\"\n",
    "\n",
    "#--------------------------> parameters\n",
    "# logging information\n",
    "gene2k_env.exp_name = f\"{model_name}_{task_name}\"\n",
    "gene2k_env.project = \"g0_arrest\" \n",
    "gene2k_env.entity = \"cell-x\"    # make sure it's initialized to an existing wandb entity\n",
    "\n",
    "#----------------> cohort\n",
    "gene2k_env.cohort_para.localcohort_name = \"BRCA\" \n",
    "gene2k_env.cohort_para.task_name = task_name\n",
    "gene2k_env.cohort_para.cohort_file = f'local_cohort_{gene2k_env.cohort_para.localcohort_name}.csv' # e.g. local_cohort_BRCA.csv, this is created automatically, and contains folder, filename, slide_nb, tissue_nb, etc. \n",
    "gene2k_env.cohort_para.task_file = f'{gene2k_env.cohort_para.localcohort_name}_{gene2k_env.cohort_para.task_name}.csv' # e.g. BRCA_g0_arrest.csv, which has PatientID matched with g0_arrest labels. This is SUPPLIED by the user and assumed to be stored in the EXP/Data/ directory\n",
    "gene2k_env.cohort_para.pid_name = \"Patient_ID\"\n",
    "gene2k_env.cohort_para.targets = f'name of target_label column'  # e.g. \"g0_arrest\"  # the column name of interest\n",
    "gene2k_env.cohort_para.targets_idx = 0\n",
    "gene2k_env.cohort_para.label_dict = \"{'HRD':0,'HRP':1}\"  # SINGLE quotations for the keys, converts strings objects to binary values\n",
    "#debug_env.cohort_para.update_localcohort = True\n",
    "#----------------> pre-processing\n",
    "#----------------> dataset\n",
    "gene2k_env.dataset_para.dataset_name = f\"BRCA_{task_name}\"\n",
    "gene2k_env.dataset_para.concepts = [\"slide\",\"patch\",\"feature\"] # default ['slide', 'tissue', 'patch', 'feature'] in this ORDER\n",
    "gene2k_env.dataset_para.split_ratio = [0.8,0.2]                # dataset split ratio which must sum to one, and training ratio is greater than testing\n",
    "#----------------> model\n",
    "gene2k_env.trainer_para.model_name = model_name\n",
    "gene2k_env.trainer_para.model_para = model_para_settings[model_name]\n",
    "#----------------> trainer or analyzer\n",
    "gene2k_env.trainer_para.backbone_name = \"resnet18\"\n",
    "gene2k_env.trainer_para.additional_pl_paras.update({\"accumulate_grad_batches\":8})\n",
    "gene2k_env.trainer_para.label_format = \"int\"#\"one_hot\" \n",
    "gene2k_env.trainer_para.use_pre_calculated = True ### FOR LOADING COMPUTED FEATURES\n",
    "\n",
    "#k_fold = None\n",
    "#--------------------------> init machine and person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_cohort_loc = \"Path/to/BRCA_machine_config.pkl\"\n",
    "with open(machine_cohort_loc, \"rb\") as f:   # Unpickling\n",
    "    [data_locs,exp_locs,machine,user] = pickle.load(f)\n",
    "gene2k_env.data_locs = data_locs\n",
    "gene2k_env.exp_locs = exp_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize wandb (once is enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_dir = 'path/to API.env/'                    # We assume you store your API keys in a .env file\n",
    "# load_dotenv(dotenv_path=f\"{api_dir}API.env\")\n",
    "# user.wandb_api_key = os.getenv(\"WANDB_API_KEY\") # We assume your wandb API key is named as WANDB_API_KEY in the API.env file                             # should have the api key if machine_config.ipynb was run without problems\n",
    "user.wandb_api_key                                # should have the API key if the machine_config.ipynb notebook was run without issues\n",
    "\n",
    "wandb.init(project=gene2k_env.project, \n",
    "           entity=gene2k_env.entity,\n",
    "           api_key=user.wandb_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisation and Training\n",
    "\n",
    "The third and final section of the notebook covers model definition and training. This includes defining the MIL model using the parameters defined in the first section, and training the model using the dataloaders created in the second section.\n",
    "\n",
    "After training is complete, the notebook will also demonstrate how to evaluate the trained model on a validation set and make predictions on new whole-slide images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"setup experiment\")\n",
    "from HistoMIL.EXP.workspace.experiment import Experiment\n",
    "exp = Experiment(env_paras=gene2k_env)\n",
    "exp.setup_machine(machine=machine,user=user)\n",
    "logging.info(\"setup data\")\n",
    "exp.init_cohort()\n",
    "logging.info(\"setup trainer..\")\n",
    "exp.setup_experiment(main_data_source=\"slide\",\n",
    "                    need_train=True)\n",
    "\n",
    "exp.exp_worker.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Histo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
