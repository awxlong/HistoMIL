{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/awxlong/Desktop/my-studies/hpc_exps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcam.methods import SmoothGradCAMpp\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.stats import rankdata\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "import torch \n",
    "from HistoMIL.MODEL.Image.MIL.TransMIL.pl import pl_TransMIL\n",
    "from HistoMIL.MODEL.Image.MIL.TransMIL.paras import  TransMILParas\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.CLAM.pl import pl_CLAM\n",
    "from HistoMIL.MODEL.Image.MIL.CLAM.paras import  CLAMParas\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.Transformer.pl import pl_Transformer\n",
    "from HistoMIL.MODEL.Image.MIL.Transformer.paras import  TransformerParas\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.AttentionMIL.pl import pl_AttentionMIL\n",
    "from HistoMIL.MODEL.Image.MIL.AttentionMIL.paras import  AttentionMILParas\n",
    "\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.DTFD_MIL.pl import pl_DTFD_MIL\n",
    "from HistoMIL.MODEL.Image.MIL.DTFD_MIL.paras import  DTFD_MILParas\n",
    "\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.CAMIL.pl import pl_CAMIL\n",
    "from HistoMIL.MODEL.Image.MIL.CAMIL.paras import  CAMILParas\n",
    "\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.TransMILRegression.pl import pl_TransMILRegression\n",
    "from HistoMIL.MODEL.Image.MIL.TransMILRegression.paras import  TransMILRegressionParas\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.GraphTransformer.pl import pl_GraphTransformer\n",
    "from HistoMIL.MODEL.Image.MIL.GraphTransformer.paras import GraphTransformerParas\n",
    "\n",
    "from HistoMIL.DATA.Slide.concepts.WholeSlideImage import WholeSlideImageHeatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_object = WholeSlideImageHeatmap(path='/Users/awxlong/Desktop/my-studies/temp_data/CRC/TCGA-CRC/8472de58-9075-4534-b00b-3a87ba2158da/TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs')\n",
    "label = 0\n",
    "# pkl_path = '/Users/awxlong/Desktop/my-studies/temp_data/CRC/Tissue/tcga_folder_1.TCGA-A8-A085-01Z-00-DX1.2B52D1B8-5AD4-4BD6-ADF7-9D65B8EE2622.svs.pkl'\n",
    "pkl_path = '/Users/awxlong/Desktop/my-studies/temp_data/CRC/Tissue/8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.pkl'\n",
    "with open(pkl_path, 'rb') as f:\n",
    "    wsi_tissue = pickle.load(f)\n",
    "wsi_coords = h5py.File('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Patch/224_224/8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.h5')\n",
    "wsi_coords = wsi_coords['coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_object.contours_tissue = wsi_tissue['tissue']\n",
    "wsi_object.holes_tissue = wsi_tissue['holes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n",
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n",
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n",
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n",
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n",
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PPEG positional encoding\n",
      "Using PPEG positional encoding\n",
      "Using PPEG positional encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PPEG positional encoding\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_TRANSMIL_PARAS = TransMILParas()\n",
    "pl_model = pl_TransMIL(paras=DEFAULT_TRANSMIL_PARAS)\n",
    "pl_checkpoint = '/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_transmil_uni_32epochs_reruncv=1_epoch=07-auroc_val=0.81.ckpt'\n",
    "\n",
    "DEFAULT_CLAM_PARAS = CLAMParas()\n",
    "clam = pl_CLAM(paras=DEFAULT_CLAM_PARAS)\n",
    "clam_chekpoint = '/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_transmil_uni_32epochs_reruncv=1_epoch=07-auroc_val=0.81.ckpt'\n",
    "pl_model = pl_model.load_from_checkpoint(pl_checkpoint)\n",
    "clam = clam.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_clam_uni_42epochs_cv5cv=1_epoch=14-auroc_val=0.83.ckpt')\n",
    "\n",
    "DEFAULT_TRANSFORMER_PARAS = TransformerParas()\n",
    "transformer = pl_Transformer(paras=DEFAULT_TRANSFORMER_PARAS)\n",
    "transformer = transformer.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_transformer_uni_8epochs_smaller_0711_0940cv=2_epoch=07-auroc=0.00.ckpt')\n",
    "\n",
    "DEFAULT_ATTMIL_PARAS = AttentionMILParas()\n",
    "attmil = pl_AttentionMIL(paras=DEFAULT_ATTMIL_PARAS, dataset_paras=None)\n",
    "attmil = attmil.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/attentionMIL_uni_32epoch_reruncv=2_epoch=31-auroc_val=0.70.ckpt')\n",
    "\n",
    "\n",
    "# DEFAULT_DTFD_PARAS = DTFD_MILParas()\n",
    "# dtfd = pl_DTFD_MIL(paras=DEFAULT_DTFD_PARAS)\n",
    "# dtfd = dtfd.to(torch.device('cpu'))\n",
    "# dtfd = dtfd.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_dtfd_uni_42epochs_cv5_multisteplrcv=1_epoch=35-auroc_val=0.84.ckpt', map_location=torch.device('cpu'))\n",
    "\n",
    "DEFAULT_CAMIL_PARAS = CAMILParas()\n",
    "camil = pl_CAMIL(paras=DEFAULT_CAMIL_PARAS)\n",
    "camil = camil.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_camil_uni_30epochs_cv5_reducelronplateucv=1_epoch=07-auroc_val=0.82.ckpt')\n",
    "\n",
    "\n",
    "DEFAULT_TRANSMILREG_PARAS = TransMILRegressionParas()\n",
    "transmil_regression = pl_TransMILRegression(paras=DEFAULT_TRANSMILREG_PARAS)\n",
    "transmil_regression = transmil_regression.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_transmil_uni_regression_32epochs_cv5_rerun_f1_monitorcv=4_epoch=02-auroc_val=0.39.ckpt')\n",
    "\n",
    "\n",
    "DEFAULT_GRAPHTRANSFORMER_PARAS = GraphTransformerParas()\n",
    "graphtransformer = pl_GraphTransformer(paras=DEFAULT_GRAPHTRANSFORMER_PARAS)\n",
    "graphtransformer = graphtransformer.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_graphtransformer_uni_42epochs_cv5_multisteplrcv=2_epoch=00-auroc_val=0.72.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam_extractor = SmoothGradCAMpp(pl_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/uni/8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.pt')\n",
    "# feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/uni/0bdf189f-bfe3-4587-8303-d1905a3822e4.TCGA-F4-6856-01Z-00-DX1.2872c7b5-b94d-4147-ad90-69f88668135a.svs.pt')\n",
    "# feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/uni/35e53d2b-8e11-4e5c-ac45-87ea9a504c4b.TCGA-F5-6812-01Z-00-DX1.47a702ae-cfc6-48ea-8ac0-f9ec194cfb6e.svs.pt')\n",
    "# feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet50/8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.pt')\n",
    "# feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/74a7137c-ea54-477d-a505-2cf147f3cf24.TCGA-AA-A029-01Z-00-DX1.36BA3129-431D-4AE5-98E6-BA064D0B5062.svs.pt')\n",
    "# feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/prov-gigapath/1ce88052-f889-4630-871d-09de5c5ad369.TCGA-NH-A6GB-01Z-00-DX1.AD90C375-54ED-4EE4-A537-59A2E3FE4BCD.svs.pt')\n",
    "feat_adj_matrix = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet50_adj_matrix/8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.pt8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5668, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 5667, 5667, 5667],\n",
       "                       [   0,    1,    9,  ..., 5665, 5646, 5660]]),\n",
       "       values=tensor([0.8699, 0.8188, 0.8699,  ..., 0.8725, 0.7761, 0.7711]),\n",
       "       size=(5668, 5668), nnz=45344, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feat_embedding.shape)\n",
    "feat_embedding[0:3, :]\n",
    "feat_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits, Y_prob, Y_hat, A, _  = clam.model(feat_embedding[None])\n",
    "# # A = torch.rand(1, 5668, 512)\n",
    "# A = A.view(-1, 1).cpu().detach().numpy() \n",
    "# A.shape\n",
    "\n",
    "# logits, Y_prob, Y_hat, A = pl_model.model.infer(feat_embedding[None])\n",
    "\n",
    "# A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5777, 512])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "with torch.no_grad():\n",
    "    # logits, Y_prob, Y_hat, A, _  = clam.model(feat_embedding[None])\n",
    "    # logits, Y_prob, Y_hat, A = pl_model.model.infer(feat_embedding[None])\n",
    "    # logits, Y_prob, Y_hat, A = transformer.model.infer(feat_embedding[None])\n",
    "    # logits, Y_prob, Y_hat, A = attmil.model.infer(feat_embedding[None])\n",
    "    # logits, Y_prob, Y_hat, A = dtfd.model.infer(feat_embedding[None])\n",
    "    with torch.amp.autocast(device_type='cpu'):\n",
    "        logits, Y_prob, Y_hat, A = camil.model.infer([feat_embedding[None], feat_adj_matrix])\n",
    "    # logits, Y_prob, Y_hat, A = transmil_regression.model.infer(feat_embedding[None])\n",
    "    # logits, Y_prob, Y_hat, A = graphtransformer.model.infer(feat_embedding[None], feat_adj_matrix)\n",
    "    if A.dim() == 3:\n",
    "        A = A.mean(-1) # aggregate attention vectors\n",
    "    Y_hat = Y_hat.item()\n",
    "    A = A.view(-1, 1).cpu().numpy() \n",
    "    probs, ids = torch.topk(Y_prob, k)\n",
    "    probs = probs[-1].cpu().numpy()\n",
    "    ids = ids[-1].cpu().numpy()\n",
    "    # return ids, preds_str, probs, A as Y_hats, Y_hats_str, Y_probs, A \n",
    "    Y_hats, Y_probs, A = ids, probs, A\n",
    "\n",
    "# if not os.path.isfile(block_map_save_path): \n",
    "            # SAVE ATTENTION SCORES WITH COORDS\n",
    "    # file = h5py.File(h5_path, \"r\")\n",
    "    # coords = file['coords'][:]\n",
    "    # file.close()\n",
    "    # asset_dict = {'attention_scores': A, 'coords': coords}\n",
    "    # block_map_save_path = save_hdf5(block_map_save_path, asset_dict, mode='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5668, 1)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape # should be roughly (#Patches e.g. 5668, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (224, 224)\n",
    "wsi_ref_downsample = wsi_object.level_downsamples[0]\n",
    "patch_custom_downsample = 1\n",
    "vis_patch_size = tuple((np.array(patch_size) * np.array(wsi_ref_downsample) * patch_custom_downsample ).astype(int))\n",
    "vis_patch_size\n",
    "overlap = 0\n",
    "top_left = None\n",
    "bot_right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_vis_args = {'convert_to_percentiles': True, 'blur': True, 'custom_downsample': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_percentiles(scores):\n",
    "    \n",
    "    scores = rankdata(scores, 'average')/len(scores) * 100   \n",
    "    return scores\n",
    "\n",
    "def top_k(scores, k, invert=False):\n",
    "    if invert:\n",
    "        top_k_ids=scores.argsort()[:k]\n",
    "    else:\n",
    "        top_k_ids=scores.argsort()[::-1][:k]\n",
    "    return top_k_ids\n",
    "\n",
    "def screen_coords(scores, coords, top_left, bot_right):\n",
    "    bot_right = np.array(bot_right)\n",
    "    top_left = np.array(top_left)\n",
    "    mask = np.logical_and(np.all(coords >= top_left, axis=1), np.all(coords <= bot_right, axis=1))\n",
    "    scores = scores[mask]\n",
    "    coords = coords[mask]\n",
    "    return scores, coords\n",
    "\n",
    "def sample_indices(scores, k, start=0.48, end=0.52, convert_to_percentile=False, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    if convert_to_percentile:\n",
    "        end_value = np.quantile(scores, end)\n",
    "        start_value = np.quantile(scores, start)\n",
    "    else:\n",
    "        end_value = end\n",
    "        start_value = start\n",
    "    score_window = np.logical_and(scores >= start_value, scores <= end_value)\n",
    "    indices = np.where(score_window)[0]\n",
    "    if len(indices) < 1:\n",
    "        return -1 \n",
    "    else:\n",
    "        return np.random.choice(indices, min(k, len(indices)), replace=False)\n",
    "\n",
    "def sample_rois(scores, coords, k=5, mode='range_sample', seed=1, score_start=0.45, score_end=0.55, top_left=None, bot_right=None):\n",
    "\n",
    "    if len(scores.shape) == 2:\n",
    "        scores = scores.flatten()\n",
    "\n",
    "    scores = to_percentiles(scores)\n",
    "    if top_left is not None and bot_right is not None:\n",
    "        scores, coords = screen_coords(scores, coords, top_left, bot_right)\n",
    "\n",
    "    if mode == 'range_sample':\n",
    "        sampled_ids = sample_indices(scores, start=score_start, end=score_end, k=k, convert_to_percentile=False, seed=seed)\n",
    "    elif mode == 'topk':\n",
    "        sampled_ids = top_k(scores, k, invert=False)\n",
    "    elif mode == 'reverse_topk':\n",
    "        sampled_ids = top_k(scores, k, invert=True)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    # pdb.set_trace()\n",
    "    coords = coords[:][sampled_ids]\n",
    "    scores = scores[sampled_ids]\n",
    "\n",
    "    asset = {'sampled_coords': coords, 'sampled_scores': scores}\n",
    "    return asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [{'name': 'topk_high_attention', 'sample': True, 'seed': 42, 'k': 15, 'mode': 'topk'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label, Y_hats[0]\n",
    "sampled_patches_dir = f\"/Users/awxlong/Desktop/my-studies/temp_data/CRC/Heatmap/Camil_UNI/\"\n",
    "patch_level = 0\n",
    "patch_size = (224, 224)\n",
    "slide_id = 'TCGA-AD-6963'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling topk_high_attention\n",
      "coord: [1168 6128] score: 100.000\n",
      "coord: [9904 1872] score: 99.982\n",
      "coord: [12144  5904] score: 99.965\n",
      "coord: [13712 12400] score: 99.947\n",
      "coord: [25136 13072] score: 99.929\n",
      "coord: [12592  6128] score: 99.912\n",
      "coord: [24688 12848] score: 99.894\n",
      "coord: [11472  3216] score: 99.876\n",
      "coord: [25136 13520] score: 99.859\n",
      "coord: [12816 13520] score: 99.841\n",
      "coord: [13488 12400] score: 99.824\n",
      "coord: [11696  4112] score: 99.806\n",
      "coord: [14832 10608] score: 99.788\n",
      "coord: [11024  3664] score: 99.771\n",
      "coord: [24240 12400] score: 99.753\n"
     ]
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    if sample['sample']:\n",
    "        tag = \"label_{}_pred_{}\".format(label, Y_hats[0])\n",
    "        sample_save_dir =  os.path.join(sampled_patches_dir, 'sampled_patches', str(tag), sample['name'])\n",
    "        os.makedirs(sample_save_dir, exist_ok=True)\n",
    "        print('sampling {}'.format(sample['name']))\n",
    "        sample_results = sample_rois(scores=A, coords=wsi_coords, k=sample['k'], mode=sample['mode'], seed=sample['seed'], \n",
    "            score_start=sample.get('score_start', 0), score_end=sample.get('score_end', 1))\n",
    "        for idx, (s_coord, s_score) in enumerate(zip(sample_results['sampled_coords'], sample_results['sampled_scores'])):\n",
    "            print('coord: {} score: {:.3f}'.format(s_coord, s_score))\n",
    "            patch = wsi_object.wsi.read_region(tuple(s_coord), patch_level, patch_size).convert('RGB')\n",
    "            patch.save(os.path.join(sample_save_dir, '{}_{}_x_{}_y_{}_a_{:.3f}.png'.format(idx, slide_id, s_coord[0], s_coord[1], s_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "creating heatmap for: \n",
      "top_left:  (0, 0) bot_right:  (31655, 25773)\n",
      "w: 7913, h: 6443\n",
      "scaled patch size:  [56 56]\n",
      "\n",
      "computing foreground tissue mask\n",
      "detected 16947260/50983459 of region as tissue\n",
      "\n",
      "computing heatmap image\n",
      "total of 5668 patches\n",
      "progress: 1132/5668\n",
      "progress: 2265/5668\n",
      "progress: 3398/5668\n",
      "progress: 4531/5668\n",
      "progress: 5664/5668\n",
      "Done\n",
      "\n",
      "computing blend\n",
      "using block size: 1024 x 1024\n"
     ]
    }
   ],
   "source": [
    "heatmap = wsi_object.visHeatmap(scores=A, coords=wsi_coords, vis_level=1,  \n",
    "                                cmap='jet', alpha=0.4, **heatmap_vis_args, \n",
    "                                binarize=False, blank_canvas=False,\n",
    "                                thresh=-1,  patch_size = vis_patch_size,\n",
    "                                overlap=overlap, top_left=top_left, bot_right = bot_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap.save(os.path.join('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Heatmap', 'graphtransformer_heatmap_uni_half_res_gauss_blur.jpg'), quality=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prov-gigapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/prov-gigapath/0b9fadfc-f3ba-4af2-899c-bf804369fd55.TCGA-A6-A567-01Z-00-DX1.F941874E-9BF7-4E8B-908C-41A638D62275.svs.pt')\n",
    "feat_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/prov-gigapath/1ce88052-f889-4630-871d-09de5c5ad369.TCGA-NH-A6GB-01Z-00-DX1.AD90C375-54ED-4EE4-A537-59A2E3FE4BCD.svs.pt')\n",
    "feat_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/prov-gigapath/fbde56ab-b131-436e-8a6d-24c1e05217ca.TCGA-CM-4743-01Z-00-DX1.f54a6355-5623-498c-96b9-2ff1de6576c6.svs.pt')\n",
    "feat_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/prov-gigapath/0acc3fcf-1209-4824-8d25-e4d0c0f2ac1b.TCGA-A6-2674-01Z-00-DX1.d301f1f5-6f4a-49e6-9c93-f4e8b7f616b8.svs.pt')\n",
    "feat_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet50/0acc3fcf-1209-4824-8d25-e4d0c0f2ac1b.TCGA-A6-2674-01Z-00-DX1.d301f1f5-6f4a-49e6-9c93-f4e8b7f616b8.svs.pt')\n",
    "feat_embedding[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet50/0ae1ce57-7580-48de-9f8b-2eff2173b538.TCGA-AH-6903-01Z-00-DX1.29f3feae-e757-4138-9f86-439fac247009.svs.pt')\n",
    "feat_embedding[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet50/0b55ba87-0994-4679-9d1c-6cc3ddfd3078.TCGA-A6-5662-01Z-00-DX1.82569684-1c31-4346-af9b-c296a020f624.svs.pt')\n",
    "feat_embedding[42]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
