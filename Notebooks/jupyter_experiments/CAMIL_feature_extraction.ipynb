{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/awxlong/Desktop/my-studies/hpc_exps/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HistoMIL.MODEL.Image.MIL.CAMIL.model import CAMIL\n",
    "from HistoMIL.MODEL.Image.MIL.CAMIL.paras import CAMILParas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from HistoMIL import logger\n",
    "import logging\n",
    "logger.setLevel(logging.INFO)\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsi_coords = h5py.File('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Patch/224_224/tcga_folder_4.TCGA-A8-A086-01Z-00-DX1.2B52D1B8-5AD4-4BD6-ADF7-9D65B8EE2623.svs.h5', 'r')\n",
    "# wsi_coords = wsi_coords['coords']\n",
    "# wsi_coords2 = h5py.File('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Patch/224_224/tcga_folder_4.TCGA-AA-A01X-01Z-00-DX1.7433F54C-2A79-467A-8FEA-638AE48F42A0.svs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsi_feats = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet18/tcga_folder_3.TCGA-A8-A086-01Z-00-DX1.2B52D1B8-5AD4-4BD6-ADF7-9D65B8EE2623.svs.pt')\n",
    "# wsi_feats2 = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet18/tcga_folder_4.TCGA-AA-A01X-01Z-00-DX1.7433F54C-2A79-467A-8FEA-638AE48F42A0.svs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = []\n",
    "# features.append(wsi_feats.cpu().numpy())\n",
    "# features.append(wsi_feats2.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.vstack(features).shape\n",
    "# wsi_coords.shape\n",
    "# wsi_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------> init machine and person\n",
    "\n",
    "machine_cohort_loc = '/Users/awxlong/Desktop/my-studies/hpc_exps/User/CRC_machine_config.pkl'#  f\"{args.cohort_dir}/User/{args.localcohort_name}_machine_config.pkl\"\n",
    "with open(machine_cohort_loc, \"rb\") as f:   # Unpickling\n",
    "    [data_locs,exp_locs,machine,user] = pickle.load(f)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hdf5(output_path, asset_dict, attr_dict=None, mode='a'):\n",
    "    file = h5py.File(output_path, mode)\n",
    "    for key, val in asset_dict.items():\n",
    "        data_shape = val.shape\n",
    "        if key not in file:\n",
    "            data_type = val.dtype\n",
    "            chunk_shape = (1,) + data_shape[1:]\n",
    "            maxshape = (None,) + data_shape[1:]\n",
    "            dset = file.create_dataset(key, shape=data_shape, maxshape=maxshape, chunks=chunk_shape, dtype=data_type)\n",
    "            dset[:] = val\n",
    "            if attr_dict is not None:\n",
    "                if key in attr_dict.keys():\n",
    "                    for attr_key, attr_val in attr_dict[key].items():\n",
    "                        dset.attrs[attr_key] = attr_val\n",
    "        else:\n",
    "            dset = file[key]\n",
    "            dset.resize(len(dset) + data_shape[0], axis=0)\n",
    "            dset[-data_shape[0]:] = val\n",
    "    file.close()\n",
    "    return output_path\n",
    "\n",
    "def compute_adj_coords(wsi_coords, wsi_feats, wsi_name, adj_coord_save_path, adj_matrix_save_path, force_recalc = False):\n",
    "        # output_path_file = os.path.join(save_path + wsi_name + '.h5')\n",
    "        # output_path_file = data_locs.abs_loc('feature') + f'{encoder}_adj_dictionary/{wsi_name}.h5'\n",
    "        if not os.path.exists(f'{adj_matrix_save_path}{wsi_name}.pt') or force_recalc: \n",
    "             \n",
    "            patch_distances = pairwise_distances(wsi_coords, metric='euclidean', n_jobs=1)\n",
    "            neighbor_indices = np.argsort(patch_distances, axis=1)[:, :16]\n",
    "            rows = np.asarray([[enum] * len(item) for enum, item in enumerate(neighbor_indices)]).ravel()\n",
    "            columns = neighbor_indices.ravel()\n",
    "            values = []\n",
    "            coords = []\n",
    "            for row, column in zip(rows, columns):\n",
    "                    m1 = np.expand_dims(wsi_feats[int(row)], axis=0)\n",
    "                    # pdb.set_trace()\n",
    "                    m2 = np.expand_dims(wsi_feats[int(column)], axis=0)\n",
    "                    value = distance.cdist(m1.reshape(1, -1), m2.reshape(1, -1), 'cosine')[0][0]\n",
    "                    values.append(value)\n",
    "                    coords.append((row, column))\n",
    "            \n",
    "            # mode = 'a'\n",
    "            values = np.reshape(values, (wsi_coords.shape[0], neighbor_indices.shape[1]))\n",
    "            \n",
    "            coords = np.array(coords)\n",
    "            \n",
    "            asset_dict = {'adj_coords': coords, 'similarities': values, 'indices': neighbor_indices}\n",
    "\n",
    "            save_hdf5(adj_coord_save_path, asset_dict, attr_dict=None)\n",
    "\n",
    "            ### compute adjacency matrix\n",
    "            values = np.nan_to_num(values)\n",
    "\n",
    "            Idx = neighbor_indices[:, :8]\n",
    "            rows = np.asarray([[enum] * len(item) for enum, item in enumerate(Idx)]).ravel()\n",
    "\n",
    "            columns = Idx.ravel()\n",
    "\n",
    "            neighbor_matrix = values[:, 1:]\n",
    "\n",
    "            normalized_matrix = preprocessing.normalize(neighbor_matrix, norm=\"l2\")\n",
    "\n",
    "            similarities = np.exp(-normalized_matrix)\n",
    "\n",
    "            values = np.concatenate((np.max(similarities, axis=1).reshape(-1, 1), similarities), axis=1)\n",
    "\n",
    "            values = values[:, :8]\n",
    "\n",
    "            values = values.ravel().tolist()\n",
    "\n",
    "            sparse_coords= list(zip(rows, columns))\n",
    "\n",
    "            # sparse_matrix = torch.sparse_coo_tensor(sparse_coords, values, (wsi_feats2.shape[0], wsi_feats2.shape[0]))\n",
    "\n",
    "            indices = torch.tensor(sparse_coords, dtype=torch.long).t()\n",
    "            # values = torch.tensor(values, dtype=torch.float32)\n",
    "            values = torch.FloatTensor(values)\n",
    "            sparse_matrix = torch.sparse.FloatTensor(indices, values, torch.Size([wsi_feats.shape[0], wsi_feats.shape[0]]))\n",
    "\n",
    "            torch.save(sparse_matrix, f'{adj_matrix_save_path}{wsi_name}.pt')\n",
    "            logger.info(f'Adjacency matrix stored at {adj_matrix_save_path}')\n",
    "        else:\n",
    "             logger.info(f'Adjacency matrix already exists at: {adj_matrix_save_path}{wsi_name}.pt')\n",
    "        # return np.array(coords), values, neighbor_indices, sparse_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name = 'CRC'\n",
    "idx = 1\n",
    "local_cohort_dir = f'{exp_locs.root}Data/local_cohort_{cohort_name}_{idx}.csv'\n",
    "df = pd.read_csv(local_cohort_dir)\n",
    "\n",
    "step_size = 224\n",
    "encoder = 'resnet18'\n",
    "\n",
    "wsi_coord_root = data_locs.abs_loc('patch') + f'{step_size}_{step_size}/'\n",
    "wsi_feats_root = data_locs.abs_loc('feature') + f'{encoder}/'\n",
    "\n",
    "h5_path_root = data_locs.abs_loc('feature') + f'{encoder}_adj_dictionary/'\n",
    "sparse_matrix_root = data_locs.abs_loc('feature') + f'{encoder}_adj_matrix/'\n",
    "\n",
    "os.makedirs(h5_path_root, exist_ok=True)\n",
    "os.makedirs(sparse_matrix_root, exist_ok=True)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    wsi_name = f'{df.loc[i, \"folder\"]}.{df.loc[i, \"filename\"]}'\n",
    "    wsi_coords_name = f'{wsi_name}.h5'\n",
    "    wsi_feats = f'{wsi_name}.pt'\n",
    "\n",
    "    wsi_coords_dir = f'{wsi_coord_root}{wsi_coords_name}'\n",
    "    wsi_feats_dir = f'{wsi_feats_root}{wsi_feats}'\n",
    "    if os.path.exists(wsi_coords_dir) and os.path.exists(wsi_feats_dir):\n",
    "            # Load the slide and its coordinates\n",
    "            wsi_coordinates = h5py.File(wsi_coords_dir)\n",
    "            wsi_coordinates = wsi_coordinates['coords']\n",
    "            wsi_features = torch.load(wsi_feats_dir)  \n",
    "            \n",
    "            adj_coords_save_path = f'{h5_path_root}{wsi_name}.h5' # data_locs.abs_loc('feature') + f'{encoder}_adj_dictionary/{wsi_name}.h5'\n",
    "            adj_matrix_save_path = f'{sparse_matrix_root}{wsi_name}.pt'\n",
    "            \n",
    "            # Process the slide and its coordinates\n",
    "            compute_adj_coords(wsi_coords = wsi_coordinates, \n",
    "                               wsi_feats = wsi_features,\n",
    "                               wsi_name = wsi_name,\n",
    "                               adj_coord_save_path=adj_matrix_save_path,\n",
    "                               adj_matrix_save_path=adj_coords_save_path,\n",
    "                               force_recalc=False\n",
    "                               )\n",
    "            \n",
    "             \n",
    "    else:\n",
    "            # Do nothing and continue to the next iteration\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "adj_coords, similarities, neighbor_indices, sparse_matrix = compute_adj_coords(wsi_coords=wsi_coords2['coords'], wsi_feats=wsi_feats2)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adj_coords.shape, similarities.shape, neighbor_indices.shape) # mismatched coords with feats # (248288, 2) (15518, 16) (15518, 16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adj_coords.shape, similarities.shape, neighbor_indices.shape) # matched coords with feats # (248288, 2) (15518, 16) (15518, 16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adj_coords.shape, similarities.shape, neighbor_indices.shape) # mismatched coords with feats # (248288, 2) (15518, 16) (15518, 16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_indices = neighbor_indices\n",
    "\n",
    "values = similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sparse_matrix, f'{sparse_matrix_save_path}temp_sparse_matrix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/uni_adj_matrix/temp_sparse_matrix.pt')\n",
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_paras = CAMILParas()\n",
    "default_paras.input_shape = 512\n",
    "rand_tensor = torch.rand(1, 1, 512)# .to('mps')\n",
    "model = CAMIL(paras=default_paras)\n",
    "# model.to('mps')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, alpha, k_alpha = model([wsi_feats2, sparse_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
