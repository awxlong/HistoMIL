{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/awxlong/Desktop/my-studies/hpc_exps/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HistoMIL.MODEL.Image.MIL.CAMIL.model import CAMIL\n",
    "from HistoMIL.MODEL.Image.MIL.CAMIL.paras import CAMILParas\n",
    "from HistoMIL.MODEL.Image.MIL.utils import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from HistoMIL import logger\n",
    "import logging\n",
    "logger.setLevel(logging.INFO)\n",
    "import pdb\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomAttentioneEff(nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             input_dim,\n",
    "#             weight_params_dim,\n",
    "#             kernel_initializer=\"xavier_uniform\",\n",
    "#             kernel_regularizer=None,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.weight_params_dim = weight_params_dim\n",
    "\n",
    "#         # Initialize weights\n",
    "#         self.wq_weight_params = nn.Parameter(torch.Tensor(input_dim, \n",
    "#                                                           weight_params_dim))\n",
    "#         self.wk_weight_params = nn.Parameter(torch.Tensor(input_dim, \n",
    "#                                                           weight_params_dim))\n",
    "\n",
    "#         # Initialize weights using the specified initializer\n",
    "#         if kernel_initializer == \"xavier_uniform\":\n",
    "#             nn.init.xavier_uniform_(self.wq_weight_params)\n",
    "#             nn.init.xavier_uniform_(self.wk_weight_params)\n",
    "#         else:\n",
    "#             # Add other initializers as needed\n",
    "#             raise ValueError(f\"Unsupported initializer: {kernel_initializer}\")\n",
    "\n",
    "#         # Regularization is typically applied during the loss computation in PyTorch\n",
    "#         self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         return self.compute_attention_scores(inputs)\n",
    "\n",
    "#     def compute_attention_scores(self, instance):\n",
    "#         ### can futher optimize this using F.scaled_dot_product \n",
    "#         # q = torch.matmul(instance, self.wq_weight_params)\n",
    "#         # k = torch.matmul(instance, self.wk_weight_params)\n",
    "\n",
    "#         # # dk = torch.tensor(k.size(-1), dtype=torch.float32)\n",
    "#         # dk = torch.tensor(k.shape[-1], dtype=torch.int32)\n",
    "#         # # pdb.set_trace()\n",
    "#         # matmul_qk = torch.matmul(q, k.transpose(-2, -1))  # (..., seq_len_q, seq_len_k)\n",
    "#         # # matmul_qk = torch.tensordot(q, k.transpose(-2, -1), dims=1) # could also be this\n",
    "        \n",
    "#         # scaled_attention_logits = matmul_qk / torch.sqrt(dk)\n",
    "\n",
    "        \n",
    "#         chunk_size = 1024  # Adjust this value based on your GPU memory\n",
    "#         q_chunks = []\n",
    "#         k_chunks = []\n",
    "        \n",
    "#         for i in range(0, instance.size(0), chunk_size):\n",
    "#             chunk = instance[i:i+chunk_size]\n",
    "#             q_chunks.append(torch.matmul(chunk, self.wq_weight_params))\n",
    "#             k_chunks.append(torch.matmul(chunk, self.wk_weight_params))\n",
    "        \n",
    "#         q = torch.cat(q_chunks, dim=0)\n",
    "#         k = torch.cat(k_chunks, dim=0)\n",
    "\n",
    "#         # Use torch.sqrt() directly on a scalar\n",
    "#         dk = torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "\n",
    "#         # Use torch.bmm for batch matrix multiplication\n",
    "#         # Reshape q and k for bmm\n",
    "#         q_reshaped = q.view(-1, q.size(-2), q.size(-1))\n",
    "#         k_reshaped = k.view(-1, k.size(-2), k.size(-1))\n",
    "        \n",
    "#         # Compute attention scores in chunks\n",
    "#         attention_chunks = []\n",
    "#         for i in range(0, q_reshaped.size(0), chunk_size):\n",
    "#             q_chunk = q_reshaped[i:i+chunk_size]\n",
    "#             k_chunk = k_reshaped[i:i+chunk_size]\n",
    "            \n",
    "#             matmul_qk = torch.bmm(q_chunk, k_chunk.transpose(1, 2))\n",
    "#             scaled_chunk = matmul_qk / dk\n",
    "#             attention_chunks.append(scaled_chunk)\n",
    "        \n",
    "#         scaled_attention_logits = torch.cat(attention_chunks, dim=0)\n",
    "        \n",
    "#         # Reshape back to original dimensions\n",
    "#         scaled_attention_logits = scaled_attention_logits.view(q.shape[:-1] + (k.size(-2),))\n",
    "        \n",
    "#         return scaled_attention_logits\n",
    "    \n",
    "# class CustomAttention(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_dim,\n",
    "#         weight_params_dim=256,\n",
    "#         kernel_initializer=\"glorot_uniform\",\n",
    "#         kernel_regularizer=None,\n",
    "#         chunk_size=1024\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.weight_params_dim = weight_params_dim\n",
    "\n",
    "#         # Initialize weights\n",
    "#         self.wq_weight_params = nn.Parameter(torch.Tensor(input_dim, \n",
    "#                                                           weight_params_dim))\n",
    "#         self.wk_weight_params = nn.Parameter(torch.Tensor(input_dim, \n",
    "#                                                           weight_params_dim))\n",
    "#         # Set initializer\n",
    "#         self.kernel_initializer = kernel_initializer\n",
    "\n",
    "#         # Set regularizer (L2 regularization in PyTorch)\n",
    "#         self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "#         # self.chunk_size = chunk_size\n",
    "#         self.reset_parameters()\n",
    "#     def reset_parameters(self):\n",
    "#         if self.kernel_initializer == \"glorot_uniform\":\n",
    "#             nn.init.xavier_uniform_(self.wq_weight_params)\n",
    "#             nn.init.xavier_uniform_(self.wk_weight_params)\n",
    "#         else:\n",
    "#             nn.init.kaiming_uniform_(self.wq_weight_params, a=torch.sqrt(5))\n",
    "#             nn.init.kaiming_uniform_(self.wk_weight_params, a=torch.sqrt(5))\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "        \n",
    "        \n",
    "#         attention_weights = self.compute_attention_scores(inputs)\n",
    "#         return attention_weights\n",
    "\n",
    "#     def compute_attention_scores(self, instance):\n",
    "#         q = torch.matmul(instance, self.wq_weight_params)\n",
    "#         k = torch.matmul(instance, self.wk_weight_params)\n",
    "\n",
    "#         # dk = torch.tensor(k.size(-1), dtype=torch.float32)\n",
    "#         dk = torch.tensor(k.shape[-1], dtype=torch.int32)\n",
    "#         # pdb.set_trace()\n",
    "#         matmul_qk = torch.matmul(q, k.transpose(-2, -1))  # (..., seq_len_q, seq_len_k)\n",
    "#         # matmul_qk = torch.tensordot(q, k.transpose(-2, -1), dims=1) # could also be this\n",
    "        \n",
    "#         scaled_attention_logits = matmul_qk / torch.sqrt(dk)\n",
    "#         return scaled_attention_logits\n",
    "    \n",
    "#     def compute_attention_scores_eff(self, instance):\n",
    "        \n",
    "#         chunk_size = 1024  # Adjust this value based on your GPU memory\n",
    "#         q_chunks = []\n",
    "#         k_chunks = []\n",
    "        \n",
    "#         for i in range(0, instance.size(0), chunk_size):\n",
    "#             chunk = instance[i:i+chunk_size]\n",
    "#             q_chunks.append(torch.matmul(chunk, self.wq_weight_params))\n",
    "#             k_chunks.append(torch.matmul(chunk, self.wk_weight_params))\n",
    "        \n",
    "#         q = torch.cat(q_chunks, dim=0)\n",
    "#         k = torch.cat(k_chunks, dim=0)\n",
    "\n",
    "#         # Use torch.sqrt() directly on a scalar\n",
    "#         dk = torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "\n",
    "#         # Use torch.bmm for batch matrix multiplication\n",
    "#         # Reshape q and k for bmm\n",
    "#         q_reshaped = q.view(-1, q.size(-2), q.size(-1))\n",
    "#         k_reshaped = k.view(-1, k.size(-2), k.size(-1))\n",
    "        \n",
    "#         # Compute attention scores in chunks\n",
    "#         attention_chunks = []\n",
    "#         for i in range(0, q_reshaped.size(0), chunk_size):\n",
    "#             q_chunk = q_reshaped[i:i+chunk_size]\n",
    "#             k_chunk = k_reshaped[i:i+chunk_size]\n",
    "            \n",
    "#             matmul_qk = torch.bmm(q_chunk, k_chunk.transpose(1, 2))\n",
    "#             scaled_chunk = matmul_qk / dk\n",
    "#             attention_chunks.append(scaled_chunk)\n",
    "        \n",
    "#         scaled_attention_logits = torch.cat(attention_chunks, dim=0)\n",
    "        \n",
    "#         # Reshape back to original dimensions\n",
    "#         scaled_attention_logits = scaled_attention_logits.view(q.shape[:-1] + (k.size(-2),))\n",
    "        \n",
    "#         return scaled_attention_logits\n",
    "    \n",
    "#     def extra_repr(self):\n",
    "#         return f'weight_params_dim={self.weight_params_dim}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor = torch.rand((1, 420, 1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mdl = CustomAttention(input_dim=1024, weight_params_dim=256)\n",
    "# mdl1 = CustomAttentioneEff(input_dim = 1024, weight_params_dim=256)\n",
    "# mdl2 = CustomAttention(input_dim = 1024, weight_params_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_coords = np.random.rand(50000, 2)  # Example coordinates, adjust as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_in_batches(X, batch_size=1000, dtype=np.float32):\n",
    "    n = X.shape[0]\n",
    "    for i in range(0, n, batch_size):\n",
    "        end = min(i + batch_size, n)\n",
    "        batch_distances = pairwise_distances(X[i:end], X, metric='euclidean')\n",
    "        yield batch_distances\n",
    "\n",
    "# Usage\n",
    "n_neighbors = 16\n",
    "neighbor_indices1 = np.empty((wsi_coords.shape[0], n_neighbors), dtype=np.int32)\n",
    "\n",
    "for idx, batch_patch_distance in enumerate(compute_distances_in_batches(wsi_coords)):\n",
    "    start_idx = idx * 1000\n",
    "    end_idx = min((idx + 1) * 1000, wsi_coords.shape[0])\n",
    "    \n",
    "    for i, distances in enumerate(batch_patch_distance):\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        neighbor_indices1[start_idx + i] = sorted_indices[:n_neighbors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_indices1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_distances = pairwise_distances(wsi_coords, metric='euclidean', n_jobs=1)\n",
    "neighbor_indices2 = np.argsort(patch_distances, axis=1)[:, :16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_indices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert neighbor_indices1 == neighbor_indices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsi_coords = h5py.File('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Patch/224_224/tcga_folder_4.TCGA-A8-A086-01Z-00-DX1.2B52D1B8-5AD4-4BD6-ADF7-9D65B8EE2623.svs.h5', 'r')\n",
    "# wsi_coords = wsi_coords['coords']\n",
    "# wsi_coords2 = h5py.File('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Patch/224_224/tcga_folder_4.TCGA-AA-A01X-01Z-00-DX1.7433F54C-2A79-467A-8FEA-638AE48F42A0.svs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsi_feats = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet18/tcga_folder_3.TCGA-A8-A086-01Z-00-DX1.2B52D1B8-5AD4-4BD6-ADF7-9D65B8EE2623.svs.pt')\n",
    "# wsi_feats2 = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet18/tcga_folder_4.TCGA-AA-A01X-01Z-00-DX1.7433F54C-2A79-467A-8FEA-638AE48F42A0.svs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = []\n",
    "# features.append(wsi_feats.cpu().numpy())\n",
    "# features.append(wsi_feats2.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.vstack(features).shape\n",
    "# wsi_coords.shape\n",
    "# wsi_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------> init machine and person\n",
    "\n",
    "machine_cohort_loc = '/Users/awxlong/Desktop/my-studies/hpc_exps/User/CRC_machine_config.pkl'#  f\"{args.cohort_dir}/User/{args.localcohort_name}_machine_config.pkl\"\n",
    "with open(machine_cohort_loc, \"rb\") as f:   # Unpickling\n",
    "    [data_locs,exp_locs,machine,user] = pickle.load(f)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hdf5(output_path, asset_dict, attr_dict=None, mode='a'):\n",
    "    file = h5py.File(output_path, mode)\n",
    "    for key, val in asset_dict.items():\n",
    "        data_shape = val.shape\n",
    "        if key not in file:\n",
    "            data_type = val.dtype\n",
    "            chunk_shape = (1,) + data_shape[1:]\n",
    "            maxshape = (None,) + data_shape[1:]\n",
    "            dset = file.create_dataset(key, shape=data_shape, maxshape=maxshape, chunks=chunk_shape, dtype=data_type)\n",
    "            dset[:] = val\n",
    "            if attr_dict is not None:\n",
    "                if key in attr_dict.keys():\n",
    "                    for attr_key, attr_val in attr_dict[key].items():\n",
    "                        dset.attrs[attr_key] = attr_val\n",
    "        else:\n",
    "            dset = file[key]\n",
    "            dset.resize(len(dset) + data_shape[0], axis=0)\n",
    "            dset[-data_shape[0]:] = val\n",
    "    file.close()\n",
    "    return output_path\n",
    "\n",
    "def compute_adj_coords(wsi_coords, wsi_feats, wsi_name, adj_coord_save_path, adj_matrix_save_path, force_recalc = False):\n",
    "        # output_path_file = os.path.join(save_path + wsi_name + '.h5')\n",
    "        # output_path_file = data_locs.abs_loc('feature') + f'{encoder}_adj_dictionary/{wsi_name}.h5'\n",
    "        if not os.path.exists(f'{adj_matrix_save_path}{wsi_name}.pt') or force_recalc: \n",
    "             \n",
    "            patch_distances = pairwise_distances(wsi_coords, metric='euclidean', n_jobs=1)\n",
    "            neighbor_indices = np.argsort(patch_distances, axis=1)[:, :16]\n",
    "            rows = np.asarray([[enum] * len(item) for enum, item in enumerate(neighbor_indices)]).ravel()\n",
    "            columns = neighbor_indices.ravel()\n",
    "            values = []\n",
    "            coords = []\n",
    "            for row, column in zip(rows, columns):\n",
    "                    m1 = np.expand_dims(wsi_feats[int(row)], axis=0)\n",
    "                    # pdb.set_trace()\n",
    "                    m2 = np.expand_dims(wsi_feats[int(column)], axis=0)\n",
    "                    value = distance.cdist(m1.reshape(1, -1), m2.reshape(1, -1), 'cosine')[0][0]\n",
    "                    values.append(value)\n",
    "                    coords.append((row, column))\n",
    "            \n",
    "            # mode = 'a'\n",
    "            values = np.reshape(values, (wsi_coords.shape[0], neighbor_indices.shape[1]))\n",
    "            \n",
    "            coords = np.array(coords)\n",
    "            \n",
    "            asset_dict = {'adj_coords': coords, 'similarities': values, 'indices': neighbor_indices}\n",
    "\n",
    "            save_hdf5(adj_coord_save_path, asset_dict, attr_dict=None)\n",
    "\n",
    "            ### compute adjacency matrix\n",
    "            values = np.nan_to_num(values)\n",
    "\n",
    "            Idx = neighbor_indices[:, :8]\n",
    "            rows = np.asarray([[enum] * len(item) for enum, item in enumerate(Idx)]).ravel()\n",
    "\n",
    "            columns = Idx.ravel()\n",
    "\n",
    "            neighbor_matrix = values[:, 1:]\n",
    "\n",
    "            normalized_matrix = preprocessing.normalize(neighbor_matrix, norm=\"l2\")\n",
    "\n",
    "            similarities = np.exp(-normalized_matrix)\n",
    "\n",
    "            values = np.concatenate((np.max(similarities, axis=1).reshape(-1, 1), similarities), axis=1)\n",
    "\n",
    "            values = values[:, :8]\n",
    "\n",
    "            values = values.ravel().tolist()\n",
    "\n",
    "            sparse_coords= list(zip(rows, columns))\n",
    "\n",
    "            # sparse_matrix = torch.sparse_coo_tensor(sparse_coords, values, (wsi_feats2.shape[0], wsi_feats2.shape[0]))\n",
    "\n",
    "            indices = torch.tensor(sparse_coords, dtype=torch.long).t()\n",
    "            # values = torch.tensor(values, dtype=torch.float32)\n",
    "            values = torch.FloatTensor(values)\n",
    "            sparse_matrix = torch.sparse.FloatTensor(indices, values, torch.Size([wsi_feats.shape[0], wsi_feats.shape[0]]))\n",
    "\n",
    "            torch.save(sparse_matrix, f'{adj_matrix_save_path}{wsi_name}.pt')\n",
    "            logger.info(f'Adjacency matrix stored at {adj_matrix_save_path}')\n",
    "        else:\n",
    "             logger.info(f'Adjacency matrix already exists at: {adj_matrix_save_path}{wsi_name}.pt')\n",
    "        # return np.array(coords), values, neighbor_indices, sparse_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name = 'CRC'\n",
    "idx = 1\n",
    "local_cohort_dir = f'{exp_locs.root}Data/local_cohort_{cohort_name}_{idx}.csv'\n",
    "df = pd.read_csv(local_cohort_dir)\n",
    "\n",
    "step_size = 224\n",
    "encoder = 'resnet18'\n",
    "\n",
    "wsi_coord_root = data_locs.abs_loc('patch') + f'{step_size}_{step_size}/'\n",
    "wsi_feats_root = data_locs.abs_loc('feature') + f'{encoder}/'\n",
    "\n",
    "h5_path_root = data_locs.abs_loc('feature') + f'{encoder}_adj_dictionary/'\n",
    "sparse_matrix_root = data_locs.abs_loc('feature') + f'{encoder}_adj_matrix/'\n",
    "\n",
    "os.makedirs(h5_path_root, exist_ok=True)\n",
    "os.makedirs(sparse_matrix_root, exist_ok=True)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    wsi_name = f'{df.loc[i, \"folder\"]}.{df.loc[i, \"filename\"]}'\n",
    "    wsi_coords_name = f'{wsi_name}.h5'\n",
    "    wsi_feats = f'{wsi_name}.pt'\n",
    "\n",
    "    wsi_coords_dir = f'{wsi_coord_root}{wsi_coords_name}'\n",
    "    wsi_feats_dir = f'{wsi_feats_root}{wsi_feats}'\n",
    "    if os.path.exists(wsi_coords_dir) and os.path.exists(wsi_feats_dir):\n",
    "            # Load the slide and its coordinates\n",
    "            wsi_coordinates = h5py.File(wsi_coords_dir)\n",
    "            wsi_coordinates = wsi_coordinates['coords']\n",
    "            wsi_features = torch.load(wsi_feats_dir)  \n",
    "            \n",
    "            adj_coords_save_path = f'{h5_path_root}{wsi_name}.h5' # data_locs.abs_loc('feature') + f'{encoder}_adj_dictionary/{wsi_name}.h5'\n",
    "            adj_matrix_save_path = f'{sparse_matrix_root}{wsi_name}.pt'\n",
    "            \n",
    "            # Process the slide and its coordinates\n",
    "            compute_adj_coords(wsi_coords = wsi_coordinates, \n",
    "                               wsi_feats = wsi_features,\n",
    "                               wsi_name = wsi_name,\n",
    "                               adj_coord_save_path=adj_matrix_save_path,\n",
    "                               adj_matrix_save_path=adj_coords_save_path,\n",
    "                               force_recalc=False\n",
    "                               )\n",
    "            \n",
    "             \n",
    "    else:\n",
    "            # Do nothing and continue to the next iteration\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "adj_coords, similarities, neighbor_indices, sparse_matrix = compute_adj_coords(wsi_coords=wsi_coords2['coords'], wsi_feats=wsi_feats2)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_feat = torch.rand((1, 29015, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_adj_matrix = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/uni_adj_matrix/temp_sparse_matrix.pt')\n",
    "uni_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HistoMIL.MODEL.Image.MIL.CAMIL.model import CAMIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_paras = CAMILParas()\n",
    "default_paras.input_shape = 1024\n",
    "rand_tensor = torch.rand(1, 1, 512)# .to('mps')\n",
    "model = CAMIL(paras=default_paras)\n",
    "model.to('cpu')\n",
    "\n",
    "# uni_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl = Attention(dim=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, alpha, k_alpha = model([uni_feat, uni_adj_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out\n",
    "alpha\n",
    "k_alpha.shape\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
