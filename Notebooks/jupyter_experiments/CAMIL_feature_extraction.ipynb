{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/awxlong/Desktop/my-studies/hpc_exps/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HistoMIL.MODEL.Image.MIL.CAMIL.model import CAMIL\n",
    "from HistoMIL.MODEL.Image.MIL.CAMIL.paras import CAMILParas\n",
    "from HistoMIL.MODEL.Image.MIL.utils import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from HistoMIL import logger\n",
    "import logging\n",
    "logger.setLevel(logging.INFO)\n",
    "import pdb\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_matrix(coords):\n",
    "    total = len(coords)\n",
    "    adj_s = np.zeros((total, total), dtype=int)\n",
    "\n",
    "    for i in range(total-1):\n",
    "        path_i = coords[i]\n",
    "        x_i, y_i = path_i # path_i.split('/')[-1].split('.')[0].split('_')\n",
    "        for j in range(i+1, total):\n",
    "            # sptial \n",
    "            path_j = coords[j]\n",
    "            x_j, y_j = path_j# .split('/')[-1].split('.')[0].split('_')\n",
    "            if abs(int(x_i)-int(x_j)) <=1 and abs(int(y_i)-int(y_j)) <= 1:\n",
    "                adj_s[i][j] = 1\n",
    "                adj_s[j][i] = 1\n",
    "\n",
    "    adj_s = torch.from_numpy(adj_s)\n",
    "    # adj_s = adj_s.cuda()\n",
    "\n",
    "    return adj_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_tensor = torch.rand((1, 420, 1024))\n",
    "wsi_coords = h5py.File('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Patch/224_224/tcga_folder_4.TCGA-A8-A086-01Z-00-DX1.2B52D1B8-5AD4-4BD6-ADF7-9D65B8EE2623.svs.h5', 'r')\n",
    "wsi_coords = wsi_coords['coords']\n",
    "# wsi_coords2 = h5py.File('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Patch/224_224/tcga_folder_4.TCGA-AA-A01X-01Z-00-DX1.7433F54C-2A79-467A-8FEA-638AE48F42A0.svs.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_wsi = wsi_coords[0:500, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unweighted_adj_coords(wsi_coords, force_recalc=True):\n",
    "    if force_recalc:\n",
    "        chunk_size = 4096\n",
    "        n_neighbors = 16\n",
    "        neighbor_indices = np.empty((wsi_coords.shape[0], n_neighbors), dtype=np.int32)\n",
    "        \n",
    "        for idx, batch_patch_distance in enumerate(compute_distances_in_batches(wsi_coords, batch_size=chunk_size)):\n",
    "            start_idx = idx * chunk_size\n",
    "            for i, distances in enumerate(batch_patch_distance):\n",
    "                sorted_indices = np.argsort(distances)\n",
    "                neighbor_indices[start_idx + i] = sorted_indices[:n_neighbors]\n",
    "\n",
    "        rows = np.asarray([[enum] * len(item) for enum, item in enumerate(neighbor_indices)]).ravel()\n",
    "        columns = neighbor_indices.ravel()\n",
    "\n",
    "        # Create unweighted adjacency matrix with binary values (0 or 1)\n",
    "        values = np.ones(len(rows), dtype=np.int8)\n",
    "\n",
    "        # Create sparse matrix\n",
    "        sparse_coords = list(zip(rows, columns))\n",
    "        indices = torch.tensor(sparse_coords, dtype=torch.long).t()\n",
    "        values = torch.FloatTensor(values)\n",
    "        sparse_matrix = torch.sparse_coo_tensor(indices, values, torch.Size([wsi_coords.shape[0], wsi_coords.shape[0]]))\n",
    "        return sparse_matrix\n",
    "def compute_distances_in_batches(coords, batch_size=4096):\n",
    "    num_patches = coords.shape[0]\n",
    "    for start_index in range(0, num_patches, batch_size):\n",
    "        end_index = min(start_index + batch_size, num_patches)\n",
    "        batch_coords = coords[start_index:end_index]\n",
    "        yield distance.cdist(batch_coords, coords, 'euclidean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_binary_adj = compute_unweighted_adj_coords(wsi_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = sparse_binary_adj.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(check, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = adj_matrix(small_wsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_in_batches(X, batch_size=1000, dtype=np.float32):\n",
    "    n = X.shape[0]\n",
    "    for i in range(0, n, batch_size):\n",
    "        end = min(i + batch_size, n)\n",
    "        batch_distances = pairwise_distances(X[i:end], X, metric='euclidean')\n",
    "        yield batch_distances\n",
    "\n",
    "# Usage\n",
    "n_neighbors = 16\n",
    "neighbor_indices1 = np.empty((wsi_coords.shape[0], n_neighbors), dtype=np.int32)\n",
    "\n",
    "for idx, batch_patch_distance in enumerate(compute_distances_in_batches(wsi_coords)):\n",
    "    start_idx = idx * 1000\n",
    "    end_idx = min((idx + 1) * 1000, wsi_coords.shape[0])\n",
    "    \n",
    "    for i, distances in enumerate(batch_patch_distance):\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        neighbor_indices1[start_idx + i] = sorted_indices[:n_neighbors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_indices1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_distances = pairwise_distances(wsi_coords, metric='euclidean', n_jobs=1)\n",
    "neighbor_indices2 = np.argsort(patch_distances, axis=1)[:, :16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_indices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert neighbor_indices1 == neighbor_indices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wsi_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsi_feats = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet18/tcga_folder_3.TCGA-A8-A086-01Z-00-DX1.2B52D1B8-5AD4-4BD6-ADF7-9D65B8EE2623.svs.pt')\n",
    "# wsi_feats2 = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet18/tcga_folder_4.TCGA-AA-A01X-01Z-00-DX1.7433F54C-2A79-467A-8FEA-638AE48F42A0.svs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = []\n",
    "# features.append(wsi_feats.cpu().numpy())\n",
    "# features.append(wsi_feats2.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.vstack(features).shape\n",
    "# wsi_coords.shape\n",
    "# wsi_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------> init machine and person\n",
    "\n",
    "machine_cohort_loc = '/Users/awxlong/Desktop/my-studies/hpc_exps/User/CRC_machine_config.pkl'#  f\"{args.cohort_dir}/User/{args.localcohort_name}_machine_config.pkl\"\n",
    "with open(machine_cohort_loc, \"rb\") as f:   # Unpickling\n",
    "    [data_locs,exp_locs,machine,user] = pickle.load(f)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hdf5(output_path, asset_dict, attr_dict=None, mode='a'):\n",
    "    file = h5py.File(output_path, mode)\n",
    "    for key, val in asset_dict.items():\n",
    "        data_shape = val.shape\n",
    "        if key not in file:\n",
    "            data_type = val.dtype\n",
    "            chunk_shape = (1,) + data_shape[1:]\n",
    "            maxshape = (None,) + data_shape[1:]\n",
    "            dset = file.create_dataset(key, shape=data_shape, maxshape=maxshape, chunks=chunk_shape, dtype=data_type)\n",
    "            dset[:] = val\n",
    "            if attr_dict is not None:\n",
    "                if key in attr_dict.keys():\n",
    "                    for attr_key, attr_val in attr_dict[key].items():\n",
    "                        dset.attrs[attr_key] = attr_val\n",
    "        else:\n",
    "            dset = file[key]\n",
    "            dset.resize(len(dset) + data_shape[0], axis=0)\n",
    "            dset[-data_shape[0]:] = val\n",
    "    file.close()\n",
    "    return output_path\n",
    "\n",
    "def compute_adj_coords(wsi_coords, wsi_feats, wsi_name, adj_coord_save_path, adj_matrix_save_path, force_recalc = False):\n",
    "        # output_path_file = os.path.join(save_path + wsi_name + '.h5')\n",
    "        # output_path_file = data_locs.abs_loc('feature') + f'{encoder}_adj_dictionary/{wsi_name}.h5'\n",
    "        if not os.path.exists(f'{adj_matrix_save_path}{wsi_name}.pt') or force_recalc: \n",
    "             \n",
    "            patch_distances = pairwise_distances(wsi_coords, metric='euclidean', n_jobs=1)\n",
    "            neighbor_indices = np.argsort(patch_distances, axis=1)[:, :16]\n",
    "            rows = np.asarray([[enum] * len(item) for enum, item in enumerate(neighbor_indices)]).ravel()\n",
    "            columns = neighbor_indices.ravel()\n",
    "            values = []\n",
    "            coords = []\n",
    "            for row, column in zip(rows, columns):\n",
    "                    m1 = np.expand_dims(wsi_feats[int(row)], axis=0)\n",
    "                    # pdb.set_trace()\n",
    "                    m2 = np.expand_dims(wsi_feats[int(column)], axis=0)\n",
    "                    value = distance.cdist(m1.reshape(1, -1), m2.reshape(1, -1), 'cosine')[0][0]\n",
    "                    values.append(value)\n",
    "                    coords.append((row, column))\n",
    "            \n",
    "            # mode = 'a'\n",
    "            values = np.reshape(values, (wsi_coords.shape[0], neighbor_indices.shape[1]))\n",
    "            \n",
    "            coords = np.array(coords)\n",
    "            \n",
    "            asset_dict = {'adj_coords': coords, 'similarities': values, 'indices': neighbor_indices}\n",
    "\n",
    "            save_hdf5(adj_coord_save_path, asset_dict, attr_dict=None)\n",
    "\n",
    "            ### compute adjacency matrix\n",
    "            values = np.nan_to_num(values)\n",
    "\n",
    "            Idx = neighbor_indices[:, :8]\n",
    "            rows = np.asarray([[enum] * len(item) for enum, item in enumerate(Idx)]).ravel()\n",
    "\n",
    "            columns = Idx.ravel()\n",
    "\n",
    "            neighbor_matrix = values[:, 1:]\n",
    "            \n",
    "            normalized_matrix = preprocessing.normalize(neighbor_matrix, norm=\"l2\")\n",
    "\n",
    "            similarities = np.exp(-normalized_matrix)\n",
    "\n",
    "            values = np.concatenate((np.max(similarities, axis=1).reshape(-1, 1), similarities), axis=1)\n",
    "\n",
    "            values = values[:, :8]\n",
    "\n",
    "            values = values.ravel().tolist()\n",
    "\n",
    "            sparse_coords= list(zip(rows, columns))\n",
    "\n",
    "            # sparse_matrix = torch.sparse_coo_tensor(sparse_coords, values, (wsi_feats2.shape[0], wsi_feats2.shape[0]))\n",
    "\n",
    "            indices = torch.tensor(sparse_coords, dtype=torch.long).t()\n",
    "            # values = torch.tensor(values, dtype=torch.float32)\n",
    "            values = torch.FloatTensor(values)\n",
    "            sparse_matrix = torch.sparse.FloatTensor(indices, values, torch.Size([wsi_feats.shape[0], wsi_feats.shape[0]]))\n",
    "\n",
    "            torch.save(sparse_matrix, f'{adj_matrix_save_path}{wsi_name}.pt')\n",
    "            logger.info(f'Adjacency matrix stored at {adj_matrix_save_path}')\n",
    "        else:\n",
    "             logger.info(f'Adjacency matrix already exists at: {adj_matrix_save_path}{wsi_name}.pt')\n",
    "        # return np.array(coords), values, neighbor_indices, sparse_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name = 'CRC'\n",
    "idx = 1\n",
    "local_cohort_dir = f'{exp_locs.root}Data/local_cohort_{cohort_name}_{idx}.csv'\n",
    "df = pd.read_csv(local_cohort_dir)\n",
    "\n",
    "step_size = 224\n",
    "encoder = 'resnet18'\n",
    "\n",
    "wsi_coord_root = data_locs.abs_loc('patch') + f'{step_size}_{step_size}/'\n",
    "wsi_feats_root = data_locs.abs_loc('feature') + f'{encoder}/'\n",
    "\n",
    "h5_path_root = data_locs.abs_loc('feature') + f'{encoder}_adj_dictionary/'\n",
    "sparse_matrix_root = data_locs.abs_loc('feature') + f'{encoder}_adj_matrix/'\n",
    "\n",
    "os.makedirs(h5_path_root, exist_ok=True)\n",
    "os.makedirs(sparse_matrix_root, exist_ok=True)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    wsi_name = f'{df.loc[i, \"folder\"]}.{df.loc[i, \"filename\"]}'\n",
    "    wsi_coords_name = f'{wsi_name}.h5'\n",
    "    wsi_feats = f'{wsi_name}.pt'\n",
    "\n",
    "    wsi_coords_dir = f'{wsi_coord_root}{wsi_coords_name}'\n",
    "    wsi_feats_dir = f'{wsi_feats_root}{wsi_feats}'\n",
    "    if os.path.exists(wsi_coords_dir) and os.path.exists(wsi_feats_dir):\n",
    "            # Load the slide and its coordinates\n",
    "            wsi_coordinates = h5py.File(wsi_coords_dir)\n",
    "            wsi_coordinates = wsi_coordinates['coords']\n",
    "            wsi_features = torch.load(wsi_feats_dir)  \n",
    "            \n",
    "            adj_coords_save_path = f'{h5_path_root}{wsi_name}.h5' # data_locs.abs_loc('feature') + f'{encoder}_adj_dictionary/{wsi_name}.h5'\n",
    "            adj_matrix_save_path = f'{sparse_matrix_root}{wsi_name}.pt'\n",
    "            \n",
    "            # Process the slide and its coordinates\n",
    "            compute_adj_coords(wsi_coords = wsi_coordinates, \n",
    "                               wsi_feats = wsi_features,\n",
    "                               wsi_name = wsi_name,\n",
    "                               adj_coord_save_path=adj_matrix_save_path,\n",
    "                               adj_matrix_save_path=adj_coords_save_path,\n",
    "                               force_recalc=False\n",
    "                               )\n",
    "            \n",
    "             \n",
    "    else:\n",
    "            # Do nothing and continue to the next iteration\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "adj_coords, similarities, neighbor_indices, sparse_matrix = compute_adj_coords(wsi_coords=wsi_coords2['coords'], wsi_feats=wsi_feats2)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class CustomAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            weight_params_dim,\n",
    "            kernel_initializer=\"xavier_uniform\",\n",
    "            kernel_regularizer=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight_params_dim = weight_params_dim\n",
    "\n",
    "        self.wq_weight_params = nn.Parameter(torch.Tensor(input_dim, weight_params_dim))\n",
    "        self.wk_weight_params = nn.Parameter(torch.Tensor(input_dim, weight_params_dim))\n",
    "\n",
    "        if kernel_initializer == \"xavier_uniform\":\n",
    "            nn.init.xavier_uniform_(self.wq_weight_params)\n",
    "            nn.init.xavier_uniform_(self.wk_weight_params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported initializer: {kernel_initializer}\")\n",
    "\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return checkpoint(self.compute_attention_scores, inputs)\n",
    "\n",
    "    def compute_attention_scores(self, instance):\n",
    "        chunk_size = 1024  # Adjust based on your GPU memory\n",
    "        \n",
    "        def process_chunk(chunk):\n",
    "            q = torch.matmul(chunk, self.wq_weight_params)\n",
    "            k = torch.matmul(chunk, self.wk_weight_params)\n",
    "            return q, k\n",
    "        \n",
    "        q_chunks = []\n",
    "        k_chunks = []\n",
    "        for i in range(0, instance.size(0), chunk_size):\n",
    "            chunk = instance[i:i+chunk_size]\n",
    "            q, k = process_chunk(chunk)\n",
    "            q_chunks.append(q)\n",
    "            k_chunks.append(k)\n",
    "        \n",
    "        q = torch.cat(q_chunks, dim=0)\n",
    "        k = torch.cat(k_chunks, dim=0)\n",
    "\n",
    "        dk = torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "\n",
    "        # Compute attention scores in chunks\n",
    "        attention_chunks = []\n",
    "        for i in range(0, q.size(0), chunk_size):\n",
    "            q_chunk = q[i:i+chunk_size]\n",
    "            k_chunk = k[i:i+chunk_size]\n",
    "            \n",
    "            matmul_qk = torch.matmul(q_chunk, k_chunk.transpose(-2, -1))\n",
    "            scaled_chunk = matmul_qk / dk\n",
    "            attention_chunks.append(scaled_chunk)\n",
    "        \n",
    "        scaled_attention_logits = torch.cat(attention_chunks, dim=0)\n",
    "        \n",
    "        return scaled_attention_logits\n",
    "    def original_comp_scores(self, instance):\n",
    "        q = torch.matmul(instance, self.wq_weight_params)\n",
    "        k = torch.matmul(instance, self.wk_weight_params)\n",
    "\n",
    "        # dk = torch.tensor(k.size(-1), dtype=torch.float32)\n",
    "        dk = torch.tensor(k.shape[-1], dtype=torch.int32)\n",
    "        # pdb.set_trace()\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))  / torch.sqrt(dk) # (..., seq_len_q, seq_len_k)\n",
    "        \n",
    "        return matmul_qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = CustomAttention(input_dim=1024, weight_params_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_feat = torch.rand((1, 420, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mdl(uni_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = mdl.original_comp_scores(uni_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_adj_matrix = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/uni_adj_matrix/temp_sparse_matrix.pt')\n",
    "uni_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_adj_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.matmul(uni_adj_matrix, uni_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(size=(1, uni_adj_matrix.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_tensor_bf=mask.new_zeros((sum(index),y.shape[2]))\n",
    "bn_tensor_af=mask.new_zeros(*y.shape)\n",
    "start_index=[]\n",
    "ssum=0\n",
    "for i in range(uni_feat.shape[0]):\n",
    "    start_index.append(ssum)\n",
    "    ssum+=index[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HistoMIL.MODEL.Image.MIL.CAMIL.model import CAMIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_paras = CAMILParas()\n",
    "default_paras.input_shape = 1024\n",
    "rand_tensor = torch.rand(1, 1, 512)# .to('mps')\n",
    "model = CAMIL(paras=default_paras)\n",
    "model.to('cpu')\n",
    "\n",
    "# uni_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl = Attention(dim=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, alpha, k_alpha = model([uni_feat, uni_adj_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out\n",
    "alpha\n",
    "k_alpha.shape\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
