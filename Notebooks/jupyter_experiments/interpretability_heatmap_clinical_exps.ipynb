{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/awxlong/Desktop/my-studies/hpc_exps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcam.methods import SmoothGradCAMpp\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.stats import rankdata\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "import torch \n",
    "from HistoMIL.MODEL.Image.MIL.TransMIL.pl import pl_TransMIL\n",
    "from HistoMIL.MODEL.Image.MIL.TransMIL.paras import  TransMILParas\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.TransMILMultimodal.pl import pl_TransMILMultimodal\n",
    "from HistoMIL.MODEL.Image.MIL.TransMILMultimodal.paras import  TransMILMultimodalParas\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.CLAM.pl import pl_CLAM\n",
    "from HistoMIL.MODEL.Image.MIL.CLAM.paras import  CLAMParas\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.Transformer.pl import pl_Transformer\n",
    "from HistoMIL.MODEL.Image.MIL.Transformer.paras import  TransformerParas\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.AttentionMIL.pl import pl_AttentionMIL\n",
    "from HistoMIL.MODEL.Image.MIL.AttentionMIL.paras import  AttentionMILParas\n",
    "\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.DTFD_MIL.pl import pl_DTFD_MIL\n",
    "from HistoMIL.MODEL.Image.MIL.DTFD_MIL.paras import  DTFD_MILParas\n",
    "\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.CAMIL.pl import pl_CAMIL\n",
    "from HistoMIL.MODEL.Image.MIL.CAMIL.paras import  CAMILParas\n",
    "\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.TransMILRegression.pl import pl_TransMILRegression\n",
    "from HistoMIL.MODEL.Image.MIL.TransMILRegression.paras import  TransMILRegressionParas\n",
    "\n",
    "from HistoMIL.MODEL.Image.MIL.GraphTransformer.pl import pl_GraphTransformer\n",
    "from HistoMIL.MODEL.Image.MIL.GraphTransformer.paras import GraphTransformerParas\n",
    "\n",
    "from HistoMIL.DATA.Slide.concepts.WholeSlideImage import WholeSlideImageHeatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_object = WholeSlideImageHeatmap(path='/Users/awxlong/Desktop/my-studies/temp_data/CRC/TCGA-CRC/8472de58-9075-4534-b00b-3a87ba2158da/TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs')\n",
    "label = 0\n",
    "# pkl_path = '/Users/awxlong/Desktop/my-studies/temp_data/CRC/Tissue/tcga_folder_1.TCGA-A8-A085-01Z-00-DX1.2B52D1B8-5AD4-4BD6-ADF7-9D65B8EE2622.svs.pkl'\n",
    "pkl_path = '/Users/awxlong/Desktop/my-studies/temp_data/CRC/Tissue/8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.pkl'\n",
    "with open(pkl_path, 'rb') as f:\n",
    "    wsi_tissue = pickle.load(f)\n",
    "wsi_coords = h5py.File('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Patch/224_224/8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.h5')\n",
    "wsi_coords = wsi_coords['coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_object.contours_tissue = wsi_tissue['tissue']\n",
    "wsi_object.holes_tissue = wsi_tissue['holes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n",
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PPEG positional encoding\n",
      "Using PPEG positional encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HistoMIL - INFO - FeatureNet:: Use: pre-calculated \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PPEG positional encoding\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_TRANSMIL_PARAS = TransMILParas()\n",
    "pl_model = pl_TransMIL(paras=DEFAULT_TRANSMIL_PARAS)\n",
    "pl_checkpoint = '/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_transmil_uni_32epochs_reruncv=1_epoch=07-auroc_val=0.81.ckpt'\n",
    "\n",
    "# DEFAULT_CLAM_PARAS = CLAMParas()\n",
    "# clam = pl_CLAM(paras=DEFAULT_CLAM_PARAS)\n",
    "# clam_chekpoint = '/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_transmil_uni_32epochs_reruncv=1_epoch=07-auroc_val=0.81.ckpt'\n",
    "# pl_model = pl_model.load_from_checkpoint(pl_checkpoint)\n",
    "# clam = clam.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_clam_uni_42epochs_cv5cv=1_epoch=14-auroc_val=0.83.ckpt')\n",
    "\n",
    "# DEFAULT_TRANSFORMER_PARAS = TransformerParas()\n",
    "# transformer = pl_Transformer(paras=DEFAULT_TRANSFORMER_PARAS)\n",
    "# transformer = transformer.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_transformer_uni_8epochs_smaller_0711_0940cv=2_epoch=07-auroc=0.00.ckpt')\n",
    "\n",
    "# DEFAULT_ATTMIL_PARAS = AttentionMILParas()\n",
    "# attmil = pl_AttentionMIL(paras=DEFAULT_ATTMIL_PARAS, dataset_paras=None)\n",
    "# attmil = attmil.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/attentionMIL_uni_32epoch_reruncv=2_epoch=31-auroc_val=0.70.ckpt')\n",
    "\n",
    "\n",
    "# # DEFAULT_DTFD_PARAS = DTFD_MILParas()\n",
    "# # dtfd = pl_DTFD_MIL(paras=DEFAULT_DTFD_PARAS)\n",
    "# # dtfd = dtfd.to(torch.device('cpu'))\n",
    "# # dtfd = dtfd.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_dtfd_uni_42epochs_cv5_multisteplrcv=1_epoch=35-auroc_val=0.84.ckpt', map_location=torch.device('cpu'))\n",
    "\n",
    "DEFAULT_CAMIL_PARAS = CAMILParas()\n",
    "camil = pl_CAMIL(paras=DEFAULT_CAMIL_PARAS)\n",
    "camil = camil.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_camil_uni_30epochs_cv5_reducelronplateucv=1_epoch=07-auroc_val=0.82.ckpt')\n",
    "\n",
    "\n",
    "# DEFAULT_TRANSMILREG_PARAS = TransMILRegressionParas()\n",
    "# transmil_regression = pl_TransMILRegression(paras=DEFAULT_TRANSMILREG_PARAS)\n",
    "# transmil_regression = transmil_regression.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_transmil_uni_regression_32epochs_cv5_rerun_f1_monitorcv=4_epoch=02-auroc_val=0.39.ckpt')\n",
    "\n",
    "\n",
    "# DEFAULT_GRAPHTRANSFORMER_PARAS = GraphTransformerParas()\n",
    "# graphtransformer = pl_GraphTransformer(paras=DEFAULT_GRAPHTRANSFORMER_PARAS)\n",
    "# graphtransformer = graphtransformer.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_graphtransformer_uni_42epochs_cv5_multisteplrcv=2_epoch=00-auroc_val=0.72.ckpt')\n",
    "\n",
    "DEFAULT_TRANSMILMULTIMODAL_PARAS = TransMILMultimodalParas()\n",
    "transmilmultimodal = pl_TransMILMultimodal(paras=DEFAULT_TRANSMILMULTIMODAL_PARAS)\n",
    "transmilmultimodal = transmilmultimodal.load_from_checkpoint('/Users/awxlong/Desktop/my-studies/hpc_exps/SavedModelsCV5/mil_transmil_uni_multimodal_32epochs_cv5_continuous_onlycv=1_epoch=13-auroc_val=0.79.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam_extractor = SmoothGradCAMpp(pl_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/uni/8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.pt')\n",
    "# feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/uni/0bdf189f-bfe3-4587-8303-d1905a3822e4.TCGA-F4-6856-01Z-00-DX1.2872c7b5-b94d-4147-ad90-69f88668135a.svs.pt')\n",
    "# feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/uni/35e53d2b-8e11-4e5c-ac45-87ea9a504c4b.TCGA-F5-6812-01Z-00-DX1.47a702ae-cfc6-48ea-8ac0-f9ec194cfb6e.svs.pt')\n",
    "# feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet50/8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.pt')\n",
    "# feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/74a7137c-ea54-477d-a505-2cf147f3cf24.TCGA-AA-A029-01Z-00-DX1.36BA3129-431D-4AE5-98E6-BA064D0B5062.svs.pt')\n",
    "# feat_embedding = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/prov-gigapath/1ce88052-f889-4630-871d-09de5c5ad369.TCGA-NH-A6GB-01Z-00-DX1.AD90C375-54ED-4EE4-A537-59A2E3FE4BCD.svs.pt')\n",
    "feat_adj_matrix = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/resnet50_adj_matrix/8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.pt8472de58-9075-4534-b00b-3a87ba2158da.TCGA-AD-6963-01Z-00-DX1.7df2e133-5f24-4c0a-b7f5-5a65fe3420c9.svs.pt')\n",
    "clinical_feats = torch.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Feature/clinical/test/TCGA-AD-6963.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5668, 1024])\n",
      "torch.Size([27]) tensor([0.4386, 0.1019, 0.0778, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 5667, 5667, 5667],\n",
       "                       [   0,    1,    9,  ..., 5665, 5646, 5660]]),\n",
       "       values=tensor([0.8699, 0.8188, 0.8699,  ..., 0.8725, 0.7761, 0.7711]),\n",
       "       size=(5668, 5668), nnz=45344, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feat_embedding.shape)\n",
    "feat_embedding[0:3, :]\n",
    "print(clinical_feats.shape, clinical_feats)\n",
    "feat_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits, Y_prob, Y_hat, A, _  = clam.model(feat_embedding[None])\n",
    "# # A = torch.rand(1, 5668, 512)\n",
    "# A = A.view(-1, 1).cpu().detach().numpy() \n",
    "# A.shape\n",
    "\n",
    "# logits, Y_prob, Y_hat, A = pl_model.model.infer(feat_embedding[None])\n",
    "\n",
    "# A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "with torch.no_grad():\n",
    "    # logits, Y_prob, Y_hat, A, _  = clam.model(feat_embedding[None])\n",
    "    # logits, Y_prob, Y_hat, A = pl_model.model.infer(feat_embedding[None])\n",
    "    # logits, Y_prob, Y_hat, A = transformer.model.infer(feat_embedding[None])\n",
    "    # logits, Y_prob, Y_hat, A = attmil.model.infer(feat_embedding[None])\n",
    "    # logits, Y_prob, Y_hat, A = dtfd.model.infer(feat_embedding[None])\n",
    "    with torch.amp.autocast(device_type='cpu'):\n",
    "        logits, Y_prob, Y_hat, A = camil.model.infer([feat_embedding[None], feat_adj_matrix])\n",
    "    # logits, Y_prob, Y_hat, A = transmil_regression.model.infer(feat_embedding[None])\n",
    "    # logits, Y_prob, Y_hat, A = graphtransformer.model.infer(feat_embedding[None], feat_adj_matrix)\n",
    "    # logits, Y_prob, Y_hat, A, clinical_gradients = transmilmultimodal.model.infer(feat_embedding[None], clinical_feats)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transmilmultimodal.model = transmilmultimodal.model.to('cpu')\n",
    "# feat_embedding = feat_embedding.to('cpu')\n",
    "# clinical_feats = clinical_feats.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits, Y_prob, Y_hat, (A, clinical_gradients) = transmilmultimodal.model.infer(feat_embedding[None], clinical_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5668])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "if A.dim() == 3:\n",
    "    A = A.mean(-1) # aggregate attention vectors\n",
    "# Y_hat = Y_hat.item()\n",
    "A = A.view(-1, 1).cpu().detach().numpy() \n",
    "probs, ids = torch.topk(Y_prob, k)\n",
    "probs = probs[-1].cpu().detach().numpy()\n",
    "ids = ids[-1].cpu().numpy()\n",
    "# return ids, preds_str, probs, A as Y_hats, Y_hats_str, Y_probs, A \n",
    "Y_hats, Y_probs, A = ids, probs, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape # should be roughly (#Patches e.g. 5668, 1)\n",
    "feat_adj_matrix.dtype\n",
    "feat_embedding.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (224, 224)\n",
    "wsi_ref_downsample = wsi_object.level_downsamples[0]\n",
    "patch_custom_downsample = 1\n",
    "vis_patch_size = tuple((np.array(patch_size) * np.array(wsi_ref_downsample) * patch_custom_downsample ).astype(int))\n",
    "vis_patch_size\n",
    "overlap = 0\n",
    "top_left = None\n",
    "bot_right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_percentiles(scores):\n",
    "    \n",
    "    scores = rankdata(scores, 'average')/len(scores) * 100   \n",
    "    return scores\n",
    "\n",
    "def top_k(scores, k, invert=False):\n",
    "    if invert:\n",
    "        top_k_ids=scores.argsort()[:k]\n",
    "    else:\n",
    "        top_k_ids=scores.argsort()[::-1][:k]\n",
    "    return top_k_ids\n",
    "\n",
    "def screen_coords(scores, coords, top_left, bot_right):\n",
    "    bot_right = np.array(bot_right)\n",
    "    top_left = np.array(top_left)\n",
    "    mask = np.logical_and(np.all(coords >= top_left, axis=1), np.all(coords <= bot_right, axis=1))\n",
    "    scores = scores[mask]\n",
    "    coords = coords[mask]\n",
    "    return scores, coords\n",
    "\n",
    "def sample_indices(scores, k, start=0.48, end=0.52, convert_to_percentile=False, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    if convert_to_percentile:\n",
    "        end_value = np.quantile(scores, end)\n",
    "        start_value = np.quantile(scores, start)\n",
    "    else:\n",
    "        end_value = end\n",
    "        start_value = start\n",
    "    score_window = np.logical_and(scores >= start_value, scores <= end_value)\n",
    "    indices = np.where(score_window)[0]\n",
    "    if len(indices) < 1:\n",
    "        return -1 \n",
    "    else:\n",
    "        return np.random.choice(indices, min(k, len(indices)), replace=False)\n",
    "\n",
    "def sample_rois(scores, coords, k=5, mode='range_sample', seed=1, score_start=0.45, score_end=0.55, top_left=None, bot_right=None):\n",
    "\n",
    "    if len(scores.shape) == 2:\n",
    "        scores = scores.flatten()\n",
    "\n",
    "    scores = to_percentiles(scores)\n",
    "    if top_left is not None and bot_right is not None:\n",
    "        scores, coords = screen_coords(scores, coords, top_left, bot_right)\n",
    "\n",
    "    if mode == 'range_sample':\n",
    "        sampled_ids = sample_indices(scores, start=score_start, end=score_end, k=k, convert_to_percentile=False, seed=seed)\n",
    "    elif mode == 'topk':\n",
    "        sampled_ids = top_k(scores, k, invert=False)\n",
    "    elif mode == 'reverse_topk':\n",
    "        sampled_ids = top_k(scores, k, invert=True)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    # pdb.set_trace()\n",
    "    coords = coords[:][sampled_ids]\n",
    "    scores = scores[sampled_ids]\n",
    "\n",
    "    asset = {'sampled_coords': coords, 'sampled_scores': scores}\n",
    "    return asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [{'name': 'topk_high_attention', 'sample': True, 'seed': 42, 'k': 15, 'mode': 'topk'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label, Y_hats[0]\n",
    "sampled_patches_dir = f\"/Users/awxlong/Desktop/my-studies/temp_data/CRC/Heatmap/Camil_UNI/\"\n",
    "patch_level = 0\n",
    "patch_size = (224, 224)\n",
    "slide_id = 'TCGA-AD-6963'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling topk_high_attention\n",
      "coord: [1168 6128] score: 100.000\n",
      "coord: [9904 1872] score: 99.982\n",
      "coord: [12144  5904] score: 99.965\n",
      "coord: [13712 12400] score: 99.947\n",
      "coord: [25136 13072] score: 99.929\n",
      "coord: [12592  6128] score: 99.912\n",
      "coord: [24688 12848] score: 99.894\n",
      "coord: [11472  3216] score: 99.876\n",
      "coord: [25136 13520] score: 99.859\n",
      "coord: [12816 13520] score: 99.841\n",
      "coord: [13488 12400] score: 99.824\n",
      "coord: [11696  4112] score: 99.806\n",
      "coord: [14832 10608] score: 99.788\n",
      "coord: [11024  3664] score: 99.771\n",
      "coord: [24240 12400] score: 99.753\n"
     ]
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    if sample['sample']:\n",
    "        tag = \"label_{}_pred_{}\".format(label, Y_hat)\n",
    "        sample_save_dir =  os.path.join(sampled_patches_dir, 'sampled_patches', str(tag), sample['name'])\n",
    "        os.makedirs(sample_save_dir, exist_ok=True)\n",
    "        print('sampling {}'.format(sample['name']))\n",
    "        sample_results = sample_rois(scores=A, coords=wsi_coords, k=sample['k'], mode=sample['mode'], seed=sample['seed'], \n",
    "            score_start=sample.get('score_start', 0), score_end=sample.get('score_end', 1))\n",
    "        for idx, (s_coord, s_score) in enumerate(zip(sample_results['sampled_coords'], sample_results['sampled_scores'])):\n",
    "            print('coord: {} score: {:.3f}'.format(s_coord, s_score))\n",
    "            patch = wsi_object.wsi.read_region(tuple(s_coord), patch_level, patch_size).convert('RGB')\n",
    "            patch.save(os.path.join(sample_save_dir, '{}_{}_x_{}_y_{}_a_{:.3f}.png'.format(idx, slide_id, s_coord[0], s_coord[1], s_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_vis_args = {'convert_to_percentiles': True, 'blur': False, 'custom_downsample': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape\n",
    "A_null = np.zeros_like(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "creating heatmap for: \n",
      "top_left:  (0, 0) bot_right:  (31655, 25773)\n",
      "w: 1978, h: 1610\n",
      "scaled patch size:  [14 14]\n",
      "\n",
      "computing foreground tissue mask\n",
      "detected 1062183/3184580 of region as tissue\n",
      "\n",
      "computing heatmap image\n",
      "total of 5668 patches\n",
      "> \u001b[0;32m/Users/awxlong/Desktop/my-studies/hpc_exps/HistoMIL/DATA/Slide/concepts/WholeSlideImage.py\u001b[0m(370)\u001b[0;36mvisHeatmap\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    368 \u001b[0;31m                    \u001b[0;31m# tissue mask block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    369 \u001b[0;31m                    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 370 \u001b[0;31m                    \u001b[0mmask_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtissue_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    371 \u001b[0;31m                    \u001b[0;31m# copy over only tissue masked portion of color block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    372 \u001b[0;31m                    \u001b[0mimg_block\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_block\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor_block\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_block\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "(14, 14, 3)\n",
      "*** NameError: name 'mask_block' is not defined\n",
      "(14, 14)\n",
      "array([[[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]],\n",
      "\n",
      "       [[ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166],\n",
      "        [ 79, 255, 166]]], dtype=uint8)\n",
      "(14, 14, 3)\n",
      "(1610, 1978, 3)\n",
      "(2,)\n",
      "array([ 31, 117])\n"
     ]
    }
   ],
   "source": [
    "heatmap = wsi_object.visHeatmap(scores=A, coords=wsi_coords, vis_level=2,  \n",
    "                                cmap='jet', alpha=0.4, **heatmap_vis_args, \n",
    "                                binarize=False, blank_canvas=False,\n",
    "                                thresh=-1,  patch_size = vis_patch_size,\n",
    "                                overlap=overlap, top_left=top_left, bot_right = bot_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap.save(os.path.join('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Heatmap', 'patches_wsi.jpg'), quality=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = wsi_object.drawPatchBoxes(scores=A, coords=wsi_coords, vis_level=2,  \n",
    "                                cmap='jet', alpha=0.4, **heatmap_vis_args, \n",
    "                                binarize=False, blank_canvas=False,\n",
    "                                thresh=-1,  patch_size = vis_patch_size,\n",
    "                                overlap=overlap, top_left=top_left, bot_right = bot_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patches.save(os.path.join('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Heatmap', 'patches_wsi.jpg'), quality=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_gradients.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.bar(range(len(clinical_gradients)), clinical_gradients)\n",
    "plt.xlabel('Clinical Feature Index')\n",
    "plt.ylabel('Integrated Gradient Value')\n",
    "plt.title('Feature Importance via Integrated Gradients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_grads = np.load('/Users/awxlong/Desktop/my-studies/temp_data/CRC/Heatmap/g0_arrest/uni/TransMILMultimodal/TCGA-AD-6964_ensemble_label_0_pred_1.0_clinical.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_grads = ensemble_grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.bar(range(len(ensemble_grads)), ensemble_grads)\n",
    "plt.xlabel('Clinical Feature Index')\n",
    "plt.ylabel('Integrated Gradient Value')\n",
    "plt.title('Feature Importance via Integrated Gradients')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
